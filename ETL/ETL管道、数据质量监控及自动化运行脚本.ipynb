{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a3e820-c872-4510-9512-3b597fd12a9a",
   "metadata": {},
   "source": [
    "# 1.导入相关依赖和配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f6aae2-398b-453b-b8bd-4e84d9a1f8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 依赖库导入完成，日志系统已初始化\n",
      "当前时间: 2025-12-02 16:19:42\n"
     ]
    }
   ],
   "source": [
    "# 代码块1：导入依赖和配置\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Optional\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 设置中文字体和样式\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 配置日志系统\n",
    "def setup_logging():\n",
    "    \"\"\"设置日志系统\"\"\"\n",
    "    logger = logging.getLogger('ETL_Pipeline')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # 创建文件处理器\n",
    "    file_handler = logging.FileHandler('etl_pipeline.log')\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # 创建控制台处理器\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # 创建格式化器\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # 添加处理器\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# 初始化日志\n",
    "logger = setup_logging()\n",
    "\n",
    "print(\"✅ 依赖库导入完成，日志系统已初始化\")\n",
    "print(f\"当前时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18ed07-16f2-4418-999d-ef7866a15e33",
   "metadata": {},
   "source": [
    "# 2.ETL管道架构设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c69fe2f3-fa7d-42cc-8f83-c00469eadc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:39:50,725 - ETL_Pipeline - INFO - [OK] ETL管道架构类定义完成\n"
     ]
    }
   ],
   "source": [
    "class EcommerceETLPipeline:\n",
    "    \"\"\"\n",
    "    电商数据ETL管道架构类\n",
    "    包含完整的数据提取、转换、加载流程\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source_dir: str = '.', db_path: str = 'ecommerce_clean.db'):\n",
    "        \"\"\"\n",
    "        初始化ETL管道\n",
    "        \n",
    "        Args:\n",
    "            source_dir: 源数据目录\n",
    "            db_path: SQLite数据库路径\n",
    "        \"\"\"\n",
    "        self.source_dir = source_dir\n",
    "        self.db_path = db_path\n",
    "        self.connection = None\n",
    "        self.execution_history = []\n",
    "        self.data_quality_metrics = {}\n",
    "        self.pipeline_config = {\n",
    "            'batch_size': 10000,\n",
    "            'enable_validation': True,\n",
    "            'enable_backup': True,\n",
    "            'cleanup_threshold': 0.95\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"ETL管道初始化 - 源目录: {source_dir}, 数据库: {db_path}\")\n",
    "    \n",
    "    def run_full_pipeline(self, generate_reports: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        运行完整的ETL管道\n",
    "        \n",
    "        Returns:\n",
    "            Dict: 包含处理结果的字典\n",
    "        \"\"\"\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\" [START] 开始运行完整ETL管道\")\n",
    "        logger.info(\"=\"*80)\n",
    "        \n",
    "        pipeline_start_time = datetime.now()\n",
    "        execution_id = f\"ETL_{pipeline_start_time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        results = {\n",
    "            'execution_id': execution_id,\n",
    "            'start_time': pipeline_start_time.isoformat(),\n",
    "            'status': 'running',\n",
    "            'stages': {},\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 阶段1: 数据提取\n",
    "            logger.info(\"[阶段1] 数据提取\")\n",
    "            stage1_start = datetime.now()\n",
    "            raw_data = self.extract_data()\n",
    "            stage1_end = datetime.now()\n",
    "            results['stages']['extraction'] = {\n",
    "                'status': 'completed',\n",
    "                'start_time': stage1_start.isoformat(),\n",
    "                'end_time': stage1_end.isoformat(),\n",
    "                'duration': (stage1_end - stage1_start).total_seconds(),\n",
    "                'tables_extracted': len(raw_data)\n",
    "            }\n",
    "            \n",
    "            # 阶段2: 数据清洗和转换\n",
    "            logger.info(\"[阶段2] 数据清洗和转换\")\n",
    "            stage2_start = datetime.now()\n",
    "            cleaned_data = self.transform_data(raw_data)\n",
    "            stage2_end = datetime.now()\n",
    "            results['stages']['transformation'] = {\n",
    "                'status': 'completed',\n",
    "                'start_time': stage2_start.isoformat(),\n",
    "                'end_time': stage2_end.isoformat(),\n",
    "                'duration': (stage2_end - stage2_start).total_seconds(),\n",
    "                'tables_cleaned': len(cleaned_data)\n",
    "            }\n",
    "            \n",
    "            # 阶段3: 数据加载\n",
    "            logger.info(\"[阶段3] 数据加载\")\n",
    "            stage3_start = datetime.now()\n",
    "            self.load_data(cleaned_data)\n",
    "            stage3_end = datetime.now()\n",
    "            results['stages']['loading'] = {\n",
    "                'status': 'completed',\n",
    "                'start_time': stage3_start.isoformat(),\n",
    "                'end_time': stage3_end.isoformat(),\n",
    "                'duration': (stage3_end - stage3_start).total_seconds()\n",
    "            }\n",
    "            \n",
    "            # 阶段4: 数据质量验证\n",
    "            logger.info(\"[阶段4] 数据质量验证\")\n",
    "            stage4_start = datetime.now()\n",
    "            quality_report = self.validate_data_quality()\n",
    "            stage4_end = datetime.now()\n",
    "            results['stages']['validation'] = {\n",
    "                'status': 'completed',\n",
    "                'start_time': stage4_start.isoformat(),\n",
    "                'end_time': stage4_end.isoformat(),\n",
    "                'duration': (stage4_end - stage4_start).total_seconds(),\n",
    "                'quality_score': quality_report.get('overall_score', 0)\n",
    "            }\n",
    "            \n",
    "            # 阶段5: 数据仓库视图创建\n",
    "            logger.info(\"[阶段5] 数据仓库视图创建\")\n",
    "            stage5_start = datetime.now()\n",
    "            self.create_data_warehouse_views()\n",
    "            stage5_end = datetime.now()\n",
    "            results['stages']['warehouse'] = {\n",
    "                'status': 'completed',\n",
    "                'start_time': stage5_start.isoformat(),\n",
    "                'end_time': stage5_end.isoformat(),\n",
    "                'duration': (stage5_end - stage5_start).total_seconds()\n",
    "            }\n",
    "            \n",
    "            # 更新结果\n",
    "            pipeline_end_time = datetime.now()\n",
    "            results.update({\n",
    "                'status': 'success',\n",
    "                'end_time': pipeline_end_time.isoformat(),\n",
    "                'total_duration': (pipeline_end_time - pipeline_start_time).total_seconds(),\n",
    "                'tables_processed': len(cleaned_data),\n",
    "                'quality_report': quality_report\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"[SUCCESS] ETL管道运行成功! 总耗时: {results['total_duration']:.2f}秒\")\n",
    "            \n",
    "            # 生成报告\n",
    "            if generate_reports:\n",
    "                self.generate_execution_report(results)\n",
    "                self.generate_dashboard(results, quality_report)\n",
    "            \n",
    "            # 保存执行历史\n",
    "            self.execution_history.append({\n",
    "                'execution_id': execution_id,\n",
    "                'timestamp': pipeline_start_time,\n",
    "                'duration': results['total_duration'],\n",
    "                'status': 'success',\n",
    "                'quality_score': quality_report.get('overall_score', 0)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ERROR] ETL管道运行失败: {str(e)}\")\n",
    "            pipeline_end_time = datetime.now()\n",
    "            results.update({\n",
    "                'status': 'failed',\n",
    "                'end_time': pipeline_end_time.isoformat(),\n",
    "                'total_duration': (pipeline_end_time - pipeline_start_time).total_seconds(),\n",
    "                'error': str(e)\n",
    "            })\n",
    "            results['errors'].append(str(e))\n",
    "            \n",
    "            # 保存失败记录\n",
    "            self.execution_history.append({\n",
    "                'execution_id': execution_id,\n",
    "                'timestamp': pipeline_start_time,\n",
    "                'duration': results['total_duration'],\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def extract_data(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        从CSV文件提取数据\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, pd.DataFrame]: 原始数据字典\n",
    "        \"\"\"\n",
    "        logger.info(\"[INFO] 开始数据提取...\")\n",
    "        \n",
    "        data_files = {\n",
    "            'customers': 'customers.csv',\n",
    "            'products': 'products.csv', \n",
    "            'time_dim': 'time_dim.csv',\n",
    "            'regions': 'regions.csv',\n",
    "            'orders': 'orders.csv',\n",
    "            'behavior_logs': 'behavior_logs.csv'\n",
    "        }\n",
    "        \n",
    "        raw_data = {}\n",
    "        extraction_stats = {}\n",
    "        \n",
    "        for table_name, file_name in data_files.items():\n",
    "            file_path = os.path.join(self.source_dir, file_name)\n",
    "            \n",
    "            try:\n",
    "                if os.path.exists(file_path):\n",
    "                    # 读取CSV文件\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # 记录提取统计\n",
    "                    extraction_stats[table_name] = {\n",
    "                        'rows': len(df),\n",
    "                        'columns': len(df.columns),\n",
    "                        'missing_values': df.isnull().sum().sum(),\n",
    "                        'duplicates': df.duplicated().sum(),\n",
    "                        'file_size_mb': os.path.getsize(file_path) / (1024 * 1024)\n",
    "                    }\n",
    "                    \n",
    "                    raw_data[table_name] = df\n",
    "                    logger.info(f\"   [OK] 已提取: {file_name} ({len(df):,} 行)\")\n",
    "                else:\n",
    "                    logger.warning(f\"   [WARN] 文件不存在: {file_path}\")\n",
    "                    raw_data[table_name] = pd.DataFrame()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"   [ERROR] 提取失败 {file_name}: {str(e)}\")\n",
    "                raw_data[table_name] = pd.DataFrame()\n",
    "        \n",
    "        # 记录提取统计\n",
    "        self.data_quality_metrics['extraction'] = extraction_stats\n",
    "        \n",
    "        logger.info(f\"[INFO] 数据提取完成，共加载 {len([d for d in raw_data.values() if not d.empty])} 个表\")\n",
    "        return raw_data\n",
    "    \n",
    "    def transform_data(self, raw_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        数据清洗和转换\n",
    "        \n",
    "        Args:\n",
    "            raw_data: 原始数据字典\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, pd.DataFrame]: 清洗后的数据字典\n",
    "        \"\"\"\n",
    "        logger.info(\"[INFO] 开始数据清洗和转换...\")\n",
    "        \n",
    "        cleaned_data = {}\n",
    "        transformation_stats = {}\n",
    "        \n",
    "        # 创建数据清洗工厂\n",
    "        cleaner_factory = {\n",
    "            'customers': self._clean_customers,\n",
    "            'products': self._clean_products,\n",
    "            'orders': self._clean_orders,\n",
    "            'time_dim': self._clean_time_dim,\n",
    "            'behavior_logs': self._clean_behavior_logs,\n",
    "            'regions': self._clean_general\n",
    "        }\n",
    "        \n",
    "        for table_name, df in raw_data.items():\n",
    "            if df.empty:\n",
    "                logger.warning(f\"[WARN] 跳过空表: {table_name}\")\n",
    "                continue\n",
    "            \n",
    "            logger.info(f\"[INFO] 清洗表: {table_name}\")\n",
    "            \n",
    "            try:\n",
    "                # 获取清洗函数\n",
    "                cleaner_func = cleaner_factory.get(table_name, self._clean_general)\n",
    "                \n",
    "                # 记录清洗前的统计\n",
    "                before_stats = {\n",
    "                    'rows': len(df),\n",
    "                    'columns': len(df.columns),\n",
    "                    'missing_values': df.isnull().sum().sum(),\n",
    "                    'duplicates': df.duplicated().sum(),\n",
    "                    'memory_mb': df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "                }\n",
    "                \n",
    "                # 执行清洗\n",
    "                cleaned_df = cleaner_func(df, table_name)\n",
    "                \n",
    "                # 记录清洗后的统计\n",
    "                after_stats = {\n",
    "                    'rows': len(cleaned_df),\n",
    "                    'columns': len(cleaned_df.columns),\n",
    "                    'missing_values': cleaned_df.isnull().sum().sum(),\n",
    "                    'duplicates': cleaned_df.duplicated().sum(),\n",
    "                    'memory_mb': cleaned_df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "                }\n",
    "                \n",
    "                # 计算改进指标\n",
    "                improvement = {\n",
    "                    'rows_removed': before_stats['rows'] - after_stats['rows'],\n",
    "                    'rows_removed_pct': ((before_stats['rows'] - after_stats['rows']) / before_stats['rows'] * 100) if before_stats['rows'] > 0 else 0,\n",
    "                    'missing_reduced': before_stats['missing_values'] - after_stats['missing_values'],\n",
    "                    'missing_reduced_pct': ((before_stats['missing_values'] - after_stats['missing_values']) / before_stats['missing_values'] * 100) if before_stats['missing_values'] > 0 else 0,\n",
    "                    'duplicates_removed': before_stats['duplicates'] - after_stats['duplicates'],\n",
    "                    'memory_reduced_mb': before_stats['memory_mb'] - after_stats['memory_mb']\n",
    "                }\n",
    "                \n",
    "                transformation_stats[table_name] = {\n",
    "                    'before': before_stats,\n",
    "                    'after': after_stats,\n",
    "                    'improvement': improvement\n",
    "                }\n",
    "                \n",
    "                cleaned_data[table_name] = cleaned_df\n",
    "                logger.info(f\"   [OK] 清洗完成: {before_stats['rows']:,} → {after_stats['rows']:,} 行 (移除: {improvement['rows_removed_pct']:.1f}%)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"   [ERROR] 清洗失败 {table_name}: {str(e)}\")\n",
    "                # 如果清洗失败，保留原始数据\n",
    "                cleaned_data[table_name] = df\n",
    "        \n",
    "        # 记录转换统计\n",
    "        self.data_quality_metrics['transformation'] = transformation_stats\n",
    "        \n",
    "        logger.info(f\"[INFO] 数据清洗完成，共处理 {len(cleaned_data)} 个表\")\n",
    "        return cleaned_data\n",
    "    \n",
    "    def _clean_general(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        通用清洗函数\n",
    "        \"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # 1. 删除完全重复的行\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "        \n",
    "        # 2. 处理ID列\n",
    "        id_columns = ['customer_id', 'order_id', 'product_id', 'log_id', 'region_id']\n",
    "        for col in id_columns:\n",
    "            if col in df_clean.columns:\n",
    "                try:\n",
    "                    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce').astype('Int64')\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # 3. 标准化列名\n",
    "        df_clean.columns = [col.strip().lower().replace(' ', '_') for col in df_clean.columns]\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def _clean_customers(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"清洗客户数据\"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # 标准化列名\n",
    "        df_clean.columns = [col.strip().lower().replace(' ', '_') for col in df_clean.columns]\n",
    "        \n",
    "        # 删除完全重复的行\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "        \n",
    "        # 处理客户ID\n",
    "        if 'customer_id' in df_clean.columns:\n",
    "            df_clean['customer_id'] = pd.to_numeric(df_clean['customer_id'], errors='coerce').astype('Int64')\n",
    "            df_clean = df_clean.dropna(subset=['customer_id'])\n",
    "        \n",
    "        # 清洗姓名\n",
    "        if 'customer_name' in df_clean.columns:\n",
    "            df_clean['customer_name'] = df_clean['customer_name'].astype(str).str.strip().str.title()\n",
    "            missing_names = df_clean['customer_name'].isna() | (df_clean['customer_name'] == '')\n",
    "            df_clean.loc[missing_names, 'customer_name'] = 'Unknown Customer'\n",
    "        \n",
    "        # 标准化性别\n",
    "        if 'gender' in df_clean.columns:\n",
    "            gender_mapping = {\n",
    "                'male': 'Male', 'M': 'Male', '男': 'Male',\n",
    "                'female': 'Female', 'F': 'Female', '女': 'Female',\n",
    "                '未知': 'Unknown', '': 'Unknown'\n",
    "            }\n",
    "            df_clean['gender'] = df_clean['gender'].astype(str).str.strip().str.lower()\n",
    "            df_clean['gender'] = df_clean['gender'].map(gender_mapping).fillna('Unknown')\n",
    "        \n",
    "        # 清洗出生年份\n",
    "        if 'birth_year' in df_clean.columns:\n",
    "            df_clean['birth_year'] = pd.to_numeric(df_clean['birth_year'], errors='coerce')\n",
    "            current_year = datetime.now().year\n",
    "            # 删除不合理的出生年份\n",
    "            mask = (df_clean['birth_year'] >= 1900) & (df_clean['birth_year'] <= current_year - 10)\n",
    "            df_clean = df_clean[mask | df_clean['birth_year'].isna()]\n",
    "            # 计算年龄\n",
    "            df_clean['age'] = current_year - df_clean['birth_year']\n",
    "        \n",
    "        # 清洗邮箱\n",
    "        if 'email' in df_clean.columns:\n",
    "            df_clean['email'] = df_clean['email'].astype(str).str.strip().str.lower()\n",
    "            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "            valid_emails = df_clean['email'].str.contains(email_pattern, na=False)\n",
    "            df_clean.loc[~valid_emails, 'email'] = None\n",
    "        \n",
    "        # 标准化国家\n",
    "        if 'country' in df_clean.columns:\n",
    "            country_mapping = {\n",
    "                'US': 'USA', 'United States': 'USA', '美国': 'USA',\n",
    "                'CA': 'Canada', 'CANADA': 'Canada',\n",
    "                'GB': 'UK', 'United Kingdom': 'UK',\n",
    "                'AU': 'Australia'\n",
    "            }\n",
    "            df_clean['country'] = df_clean['country'].astype(str).str.strip().str.upper()\n",
    "            df_clean['country'] = df_clean['country'].map(country_mapping).fillna('Other')\n",
    "        \n",
    "        # 标准化忠诚度等级\n",
    "        if 'loyalty_tier' in df_clean.columns:\n",
    "            loyalty_mapping = {\n",
    "                'bronze': 'Bronze', 'silver': 'Silver', 'gold': 'Gold', \n",
    "                'platinum': 'Platinum', 'vip': 'Platinum'\n",
    "            }\n",
    "            df_clean['loyalty_tier'] = df_clean['loyalty_tier'].astype(str).str.strip().str.lower()\n",
    "            df_clean['loyalty_tier'] = df_clean['loyalty_tier'].map(loyalty_mapping).fillna('Bronze')\n",
    "        \n",
    "        # 处理注册日期\n",
    "        if 'registration_date' in df_clean.columns:\n",
    "            df_clean['registration_date'] = pd.to_datetime(df_clean['registration_date'], errors='coerce')\n",
    "            # 删除异常日期\n",
    "            df_clean = df_clean[df_clean['registration_date'].notna()]\n",
    "        \n",
    "        # 处理数字列中的异常值\n",
    "        numeric_cols = ['total_orders', 'total_spent']\n",
    "        for col in numeric_cols:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "                # 将负数转为正数\n",
    "                df_clean[col] = df_clean[col].abs()\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def _clean_products(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"清洗产品数据\"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # 标准化列名\n",
    "        df_clean.columns = [col.strip().lower().replace(' ', '_') for col in df_clean.columns]\n",
    "        \n",
    "        # 删除完全重复的行\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "        \n",
    "        # 处理产品ID\n",
    "        if 'product_id' in df_clean.columns:\n",
    "            df_clean['product_id'] = pd.to_numeric(df_clean['product_id'], errors='coerce').astype('Int64')\n",
    "            df_clean = df_clean.dropna(subset=['product_id'])\n",
    "        \n",
    "        # 清洗产品名称\n",
    "        if 'product_name' in df_clean.columns:\n",
    "            df_clean['product_name'] = df_clean['product_name'].astype(str).str.strip().str.title()\n",
    "            df_clean['product_name'] = df_clean['product_name'].fillna('Unknown Product')\n",
    "        \n",
    "        # 标准化产品类别\n",
    "        if 'category' in df_clean.columns:\n",
    "            category_mapping = {\n",
    "                'electronics': 'Electronics', 'clothing': 'Clothing', \n",
    "                'home': 'Home', 'books': 'Books', 'sports': 'Sports'\n",
    "            }\n",
    "            df_clean['category'] = df_clean['category'].astype(str).str.strip().str.lower()\n",
    "            df_clean['category'] = df_clean['category'].map(category_mapping).fillna('Other')\n",
    "        \n",
    "        # 清洗价格\n",
    "        if 'price' in df_clean.columns:\n",
    "            df_clean['price'] = pd.to_numeric(df_clean['price'], errors='coerce')\n",
    "            # 处理异常价格\n",
    "            min_price, max_price = 0.01, 10000\n",
    "            df_clean.loc[df_clean['price'] < min_price, 'price'] = min_price\n",
    "            df_clean.loc[df_clean['price'] > max_price, 'price'] = max_price\n",
    "            # 填充缺失的价格\n",
    "            if df_clean['price'].isna().any():\n",
    "                avg_price = df_clean['price'].mean()\n",
    "                if pd.isna(avg_price):\n",
    "                    avg_price = 50\n",
    "                df_clean['price'] = df_clean['price'].fillna(avg_price)\n",
    "        \n",
    "        # 清洗库存\n",
    "        if 'stock_quantity' in df_clean.columns:\n",
    "            df_clean['stock_quantity'] = pd.to_numeric(df_clean['stock_quantity'], errors='coerce')\n",
    "            df_clean.loc[df_clean['stock_quantity'] < 0, 'stock_quantity'] = 0\n",
    "            df_clean['stock_quantity'] = df_clean['stock_quantity'].fillna(0)\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def _clean_orders(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"清洗订单数据\"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # 标准化列名\n",
    "        df_clean.columns = [col.strip().lower().replace(' ', '_') for col in df_clean.columns]\n",
    "        \n",
    "        # 删除完全重复的行\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "        \n",
    "        # 处理订单ID\n",
    "        if 'order_id' in df_clean.columns:\n",
    "            df_clean['order_id'] = pd.to_numeric(df_clean['order_id'], errors='coerce').astype('Int64')\n",
    "            df_clean = df_clean.dropna(subset=['order_id'])\n",
    "        \n",
    "        # 处理客户ID和产品ID\n",
    "        id_cols = ['customer_id', 'product_id', 'region_id']\n",
    "        for col in id_cols:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce').astype('Int64')\n",
    "        \n",
    "        # 清洗订单日期\n",
    "        if 'order_date' in df_clean.columns:\n",
    "            df_clean['order_date'] = pd.to_datetime(df_clean['order_date'], errors='coerce')\n",
    "            min_date, max_date = pd.Timestamp('2023-01-01'), pd.Timestamp('2024-12-31')\n",
    "            df_clean = df_clean[\n",
    "                (df_clean['order_date'] >= min_date) & \n",
    "                (df_clean['order_date'] <= max_date)\n",
    "            ]\n",
    "        \n",
    "        # 清洗数量\n",
    "        if 'quantity' in df_clean.columns:\n",
    "            df_clean['quantity'] = pd.to_numeric(df_clean['quantity'], errors='coerce')\n",
    "            df_clean.loc[df_clean['quantity'] <= 0, 'quantity'] = 1\n",
    "            df_clean.loc[df_clean['quantity'] > 100, 'quantity'] = 100\n",
    "            df_clean['quantity'] = df_clean['quantity'].fillna(1)\n",
    "        \n",
    "        # 清洗单价\n",
    "        if 'unit_price' in df_clean.columns:\n",
    "            df_clean['unit_price'] = pd.to_numeric(df_clean['unit_price'], errors='coerce')\n",
    "            df_clean.loc[df_clean['unit_price'] < 0, 'unit_price'] = 0\n",
    "            df_clean.loc[df_clean['unit_price'] > 10000, 'unit_price'] = 10000\n",
    "            if df_clean['unit_price'].isna().any():\n",
    "                avg_price = df_clean['unit_price'].mean()\n",
    "                if pd.isna(avg_price):\n",
    "                    avg_price = 50\n",
    "                df_clean['unit_price'] = df_clean['unit_price'].fillna(avg_price)\n",
    "        \n",
    "        # 清洗金额\n",
    "        if 'amount' in df_clean.columns:\n",
    "            df_clean['amount'] = df_clean['amount'].astype(str).str.replace('$', '', regex=False)\n",
    "            df_clean['amount'] = pd.to_numeric(df_clean['amount'], errors='coerce')\n",
    "            # 修正金额计算错误\n",
    "            calculated_amount = df_clean['unit_price'] * df_clean['quantity']\n",
    "            amount_diff = abs(df_clean['amount'] - calculated_amount)\n",
    "            mask_large_diff = amount_diff > (df_clean['amount'] * 0.01)\n",
    "            df_clean.loc[mask_large_diff, 'amount'] = calculated_amount[mask_large_diff]\n",
    "            df_clean.loc[df_clean['amount'] < 0, 'amount'] = df_clean['amount'].abs()\n",
    "        \n",
    "        # 标准化支付方式\n",
    "        if 'payment_method' in df_clean.columns:\n",
    "            payment_mapping = {\n",
    "                'credit card': 'Credit Card', 'cc': 'Credit Card',\n",
    "                'paypal': 'PayPal', 'apple pay': 'Apple Pay',\n",
    "                'google pay': 'Google Pay', '现金': 'Cash'\n",
    "            }\n",
    "            df_clean['payment_method'] = df_clean['payment_method'].astype(str).str.strip().str.lower()\n",
    "            df_clean['payment_method'] = df_clean['payment_method'].map(payment_mapping).fillna('Other')\n",
    "        \n",
    "        # 标准化订单状态\n",
    "        if 'order_status' in df_clean.columns:\n",
    "            status_mapping = {\n",
    "                'completed': 'Completed', 'shipped': 'Shipped', \n",
    "                'processing': 'Processing', 'cancelled': 'Cancelled',\n",
    "                '待处理': 'Processing', '已取消': 'Cancelled'\n",
    "            }\n",
    "            df_clean['order_status'] = df_clean['order_status'].astype(str).str.strip().str.lower()\n",
    "            df_clean['order_status'] = df_clean['order_status'].map(status_mapping).fillna('Processing')\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def _clean_time_dim(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"清洗时间维度数据\"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # 标准化列名\n",
    "        df_clean.columns = [col.strip().lower().replace(' ', '_') for col in df_clean.columns]\n",
    "        \n",
    "        # 删除完全重复的行\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "        \n",
    "        # 确保日期唯一性\n",
    "        if 'date' in df_clean.columns:\n",
    "            df_clean = df_clean.drop_duplicates(subset=['date'])\n",
    "            df_clean['date'] = pd.to_datetime(df_clean['date'], errors='coerce')\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def _clean_behavior_logs(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"清洗用户行为日志数据\"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # 标准化列名\n",
    "        df_clean.columns = [col.strip().lower().replace(' ', '_') for col in df_clean.columns]\n",
    "        \n",
    "        # 删除完全重复的行\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "        \n",
    "        # 处理日志ID\n",
    "        if 'log_id' in df_clean.columns:\n",
    "            df_clean['log_id'] = pd.to_numeric(df_clean['log_id'], errors='coerce').astype('Int64')\n",
    "            df_clean = df_clean.dropna(subset=['log_id'])\n",
    "        \n",
    "        # 处理客户ID和产品ID\n",
    "        id_cols = ['customer_id', 'product_id']\n",
    "        for col in id_cols:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce').astype('Int64')\n",
    "        \n",
    "        # 标准化行为类型\n",
    "        if 'behavior_type' in df_clean.columns:\n",
    "            behavior_mapping = {\n",
    "                'view': 'view', 'click': 'click', \n",
    "                'add_to_cart': 'add_to_cart', 'purchase': 'purchase',\n",
    "                'wishlist': 'wishlist', '浏览': 'view', 'VIEW': 'view',\n",
    "                'CLICK': 'click', '': 'view'\n",
    "            }\n",
    "            df_clean['behavior_type'] = df_clean['behavior_type'].astype(str).str.strip().str.lower()\n",
    "            df_clean['behavior_type'] = df_clean['behavior_type'].map(behavior_mapping).fillna('view')\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def connect_to_db(self):\n",
    "        \"\"\"连接到SQLite数据库\"\"\"\n",
    "        try:\n",
    "            self.connection = sqlite3.connect(self.db_path)\n",
    "            logger.info(f\"[INFO] 成功连接到数据库: {self.db_path}\")\n",
    "        except Error as e:\n",
    "            logger.error(f\"[ERROR] 数据库连接失败: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_tables(self):\n",
    "        \"\"\"创建数据库表\"\"\"\n",
    "        if not self.connection:\n",
    "            self.connect_to_db()\n",
    "        \n",
    "        cursor = self.connection.cursor()\n",
    "        \n",
    "        # 客户表\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS customers (\n",
    "            customer_id INTEGER PRIMARY KEY,\n",
    "            customer_name TEXT,\n",
    "            gender TEXT,\n",
    "            birth_year INTEGER,\n",
    "            registration_date TEXT,\n",
    "            email TEXT,\n",
    "            phone TEXT,\n",
    "            country TEXT,\n",
    "            city TEXT,\n",
    "            zip_code TEXT,\n",
    "            registration_channel TEXT,\n",
    "            loyalty_tier TEXT,\n",
    "            preferred_category TEXT,\n",
    "            avg_order_value_segment TEXT,\n",
    "            last_login_date TEXT,\n",
    "            total_orders INTEGER,\n",
    "            total_spent REAL,\n",
    "            age INTEGER\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # 产品表\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS products (\n",
    "            product_id INTEGER PRIMARY KEY,\n",
    "            product_name TEXT,\n",
    "            category TEXT,\n",
    "            subcategory TEXT,\n",
    "            brand TEXT,\n",
    "            price REAL,\n",
    "            cost_price REAL,\n",
    "            stock_quantity INTEGER,\n",
    "            supplier TEXT,\n",
    "            rating REAL,\n",
    "            review_count INTEGER,\n",
    "            created_date TEXT,\n",
    "            is_active BOOLEAN\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # 时间维度表\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS time_dim (\n",
    "            date TEXT PRIMARY KEY,\n",
    "            day INTEGER,\n",
    "            month INTEGER,\n",
    "            month_name TEXT,\n",
    "            quarter INTEGER,\n",
    "            year INTEGER,\n",
    "            day_of_week INTEGER,\n",
    "            day_name TEXT,\n",
    "            is_weekend BOOLEAN,\n",
    "            is_holiday BOOLEAN\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # 地区表\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS regions (\n",
    "            region_id INTEGER PRIMARY KEY,\n",
    "            region_name TEXT,\n",
    "            region_manager TEXT\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # 订单表\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS orders (\n",
    "            order_id INTEGER PRIMARY KEY,\n",
    "            customer_id INTEGER,\n",
    "            product_id INTEGER,\n",
    "            order_date TEXT,\n",
    "            quantity INTEGER,\n",
    "            unit_price REAL,\n",
    "            amount REAL,\n",
    "            region_id INTEGER,\n",
    "            payment_method TEXT,\n",
    "            shipping_method TEXT,\n",
    "            order_status TEXT,\n",
    "            browsing_duration_seconds INTEGER,\n",
    "            click_count INTEGER,\n",
    "            add_to_cart_count INTEGER,\n",
    "            wishlist_added BOOLEAN,\n",
    "            discount_applied REAL,\n",
    "            customer_rating INTEGER,\n",
    "            return_requested BOOLEAN,\n",
    "            FOREIGN KEY (customer_id) REFERENCES customers (customer_id),\n",
    "            FOREIGN KEY (product_id) REFERENCES products (product_id),\n",
    "            FOREIGN KEY (region_id) REFERENCES regions (region_id)\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # 行为日志表\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS behavior_logs (\n",
    "            log_id INTEGER PRIMARY KEY,\n",
    "            customer_id INTEGER,\n",
    "            product_id INTEGER,\n",
    "            behavior_type TEXT,\n",
    "            timestamp TEXT,\n",
    "            session_id TEXT,\n",
    "            device_type TEXT,\n",
    "            browser TEXT,\n",
    "            FOREIGN KEY (customer_id) REFERENCES customers (customer_id),\n",
    "            FOREIGN KEY (product_id) REFERENCES products (product_id)\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        self.connection.commit()\n",
    "        logger.info(\"[INFO] 数据库表创建完成\")\n",
    "    \n",
    "    def load_data(self, cleaned_data: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"加载清洗后的数据到数据库\"\"\"\n",
    "        logger.info(\"[INFO] 开始加载数据到数据库...\")\n",
    "        \n",
    "        if not self.connection:\n",
    "            self.connect_to_db()\n",
    "        \n",
    "        # 创建表\n",
    "        self.create_tables()\n",
    "        \n",
    "        # 按依赖顺序加载数据\n",
    "        load_order = ['customers', 'products', 'time_dim', 'regions', 'orders', 'behavior_logs']\n",
    "        \n",
    "        # 创建cleaned_data目录\n",
    "        os.makedirs('cleaned_data', exist_ok=True)\n",
    "        \n",
    "        load_stats = {}\n",
    "        \n",
    "        for table_name in load_order:\n",
    "            if table_name in cleaned_data and not cleaned_data[table_name].empty:\n",
    "                df = cleaned_data[table_name]\n",
    "                \n",
    "                try:\n",
    "                    # 保存到CSV（备份）\n",
    "                    csv_path = f'cleaned_data/cleaned_{table_name}.csv'\n",
    "                    df.to_csv(csv_path, index=False)\n",
    "                    \n",
    "                    # 插入到数据库\n",
    "                    df.to_sql(table_name, self.connection, if_exists='replace', index=False)\n",
    "                    \n",
    "                    # 记录加载统计\n",
    "                    load_stats[table_name] = {\n",
    "                        'rows_loaded': len(df),\n",
    "                        'csv_path': csv_path,\n",
    "                        'csv_size_mb': os.path.getsize(csv_path) / (1024 * 1024)\n",
    "                    }\n",
    "                    \n",
    "                    logger.info(f\"   [OK] 已加载: {table_name} ({len(df):,} 行)\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"   [ERROR] 加载失败 {table_name}: {str(e)}\")\n",
    "                    load_stats[table_name] = {\n",
    "                        'rows_loaded': 0,\n",
    "                        'error': str(e)\n",
    "                    }\n",
    "        \n",
    "        # 记录加载统计\n",
    "        self.data_quality_metrics['loading'] = load_stats\n",
    "        \n",
    "        logger.info(\"[INFO] 数据加载完成\")\n",
    "    \n",
    "    def validate_data_quality(self) -> Dict:\n",
    "        \"\"\"验证数据质量\"\"\"\n",
    "        logger.info(\"[INFO] 开始数据质量验证...\")\n",
    "        \n",
    "        if not self.connection:\n",
    "            self.connect_to_db()\n",
    "        \n",
    "        quality_report = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'tables': {},\n",
    "            'overall_score': 0,\n",
    "            'issues_found': 0,\n",
    "            'detailed_issues': []\n",
    "        }\n",
    "        \n",
    "        tables = ['customers', 'products', 'orders', 'behavior_logs', 'time_dim', 'regions']\n",
    "        \n",
    "        for table_name in tables:\n",
    "            try:\n",
    "                df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", self.connection)\n",
    "                table_report = self._validate_table_quality(df, table_name)\n",
    "                quality_report['tables'][table_name] = table_report\n",
    "                quality_report['issues_found'] += table_report['issues_count']\n",
    "                \n",
    "                # 记录详细问题\n",
    "                for issue in table_report['issues']:\n",
    "                    quality_report['detailed_issues'].append({\n",
    "                        'table': table_name,\n",
    "                        'type': issue['type'],\n",
    "                        'description': issue['description']\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"[WARN] 无法验证表 {table_name}: {str(e)}\")\n",
    "                quality_report['detailed_issues'].append({\n",
    "                    'table': table_name,\n",
    "                    'type': 'validation_error',\n",
    "                    'description': f\"验证失败: {str(e)}\"\n",
    "                })\n",
    "        \n",
    "        # 计算总体质量评分\n",
    "        if quality_report['tables']:\n",
    "            total_score = sum([r['quality_score'] for r in quality_report['tables'].values()])\n",
    "            quality_report['overall_score'] = total_score / len(quality_report['tables'])\n",
    "        \n",
    "        logger.info(f\"[INFO] 质量验证完成 - 总体评分: {quality_report['overall_score']:.1f}/100\")\n",
    "        \n",
    "        return quality_report\n",
    "    \n",
    "    def _validate_table_quality(self, df: pd.DataFrame, table_name: str) -> Dict:\n",
    "        \"\"\"验证单个表的数据质量\"\"\"\n",
    "        report = {\n",
    "            'table_name': table_name,\n",
    "            'row_count': len(df),\n",
    "            'column_count': len(df.columns),\n",
    "            'issues': [],\n",
    "            'issues_count': 0,\n",
    "            'quality_score': 100\n",
    "        }\n",
    "        \n",
    "        # 检查缺失值\n",
    "        missing_counts = df.isnull().sum()\n",
    "        missing_cols = missing_counts[missing_counts > 0]\n",
    "        \n",
    "        if len(missing_cols) > 0:\n",
    "            missing_issue = {\n",
    "                'type': 'missing_values',\n",
    "                'description': f\"{len(missing_cols)} 列有缺失值\",\n",
    "                'details': missing_cols.to_dict()\n",
    "            }\n",
    "            report['issues'].append(missing_issue)\n",
    "            report['issues_count'] += 1\n",
    "            report['quality_score'] -= 10\n",
    "        \n",
    "        # 检查重复行\n",
    "        duplicate_rows = df.duplicated().sum()\n",
    "        if duplicate_rows > 0:\n",
    "            duplicate_issue = {\n",
    "                'type': 'duplicate_rows',\n",
    "                'description': f\"{duplicate_rows} 个重复行\",\n",
    "                'details': {'duplicate_count': duplicate_rows}\n",
    "            }\n",
    "            report['issues'].append(duplicate_issue)\n",
    "            report['issues_count'] += 1\n",
    "            report['quality_score'] -= 10\n",
    "        \n",
    "        # 检查数据类型\n",
    "        type_issues = []\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                try:\n",
    "                    pd.to_numeric(df[col], errors='raise')\n",
    "                except:\n",
    "                    pass\n",
    "                else:\n",
    "                    type_issues.append(col)\n",
    "        \n",
    "        if type_issues:\n",
    "            type_issue = {\n",
    "                'type': 'data_type_inconsistency',\n",
    "                'description': f\"{len(type_issues)} 列可能有数据类型问题\",\n",
    "                'details': {'columns': type_issues}\n",
    "            }\n",
    "            report['issues'].append(type_issue)\n",
    "            report['issues_count'] += 1\n",
    "            report['quality_score'] -= 5\n",
    "        \n",
    "        # 检查订单表的异常值\n",
    "        if table_name == 'orders':\n",
    "            if 'amount' in df.columns:\n",
    "                negative_amount = (df['amount'] < 0).sum()\n",
    "                if negative_amount > 0:\n",
    "                    amount_issue = {\n",
    "                        'type': 'negative_amount',\n",
    "                        'description': f\"{negative_amount} 个订单金额为负数\",\n",
    "                        'details': {'negative_count': negative_amount}\n",
    "                    }\n",
    "                    report['issues'].append(amount_issue)\n",
    "                    report['issues_count'] += 1\n",
    "                    report['quality_score'] -= 15\n",
    "        \n",
    "        # 确保质量分数不低于0\n",
    "        report['quality_score'] = max(0, report['quality_score'])\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def create_data_warehouse_views(self):\n",
    "        \"\"\"创建数据仓库视图\"\"\"\n",
    "        try:\n",
    "            if not self.connection:\n",
    "                self.connect_to_db()\n",
    "            \n",
    "            cursor = self.connection.cursor()\n",
    "            \n",
    "            # 客户分析视图\n",
    "            cursor.execute('''\n",
    "            CREATE VIEW IF NOT EXISTS vw_customer_analysis AS\n",
    "            SELECT \n",
    "                c.customer_id,\n",
    "                c.customer_name,\n",
    "                c.country,\n",
    "                c.city,\n",
    "                c.loyalty_tier,\n",
    "                c.age,\n",
    "                COUNT(o.order_id) as total_orders,\n",
    "                SUM(o.amount) as total_spent,\n",
    "                AVG(o.amount) as avg_order_value,\n",
    "                MAX(o.order_date) as last_order_date\n",
    "            FROM customers c\n",
    "            LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
    "            GROUP BY c.customer_id, c.customer_name, c.country, c.city, c.loyalty_tier, c.age\n",
    "            ''')\n",
    "            \n",
    "            # 产品销售分析视图\n",
    "            cursor.execute('''\n",
    "            CREATE VIEW IF NOT EXISTS vw_product_sales AS\n",
    "            SELECT \n",
    "                p.product_id,\n",
    "                p.product_name,\n",
    "                p.category,\n",
    "                p.subcategory,\n",
    "                p.brand,\n",
    "                p.price,\n",
    "                COUNT(o.order_id) as units_sold,\n",
    "                SUM(o.amount) as revenue,\n",
    "                SUM(o.quantity) as total_quantity,\n",
    "                AVG(o.customer_rating) as avg_rating\n",
    "            FROM products p\n",
    "            LEFT JOIN orders o ON p.product_id = o.product_id\n",
    "            GROUP BY p.product_id, p.product_name, p.category, p.subcategory, p.brand, p.price\n",
    "            ''')\n",
    "            \n",
    "            # 月度销售视图\n",
    "            cursor.execute('''\n",
    "            CREATE VIEW IF NOT EXISTS vw_monthly_sales AS\n",
    "            SELECT \n",
    "                strftime('%Y-%m', o.order_date) as month,\n",
    "                COUNT(o.order_id) as total_orders,\n",
    "                SUM(o.amount) as total_revenue,\n",
    "                AVG(o.amount) as avg_order_value,\n",
    "                COUNT(DISTINCT o.customer_id) as unique_customers\n",
    "            FROM orders o\n",
    "            GROUP BY strftime('%Y-%m', o.order_date)\n",
    "            ORDER BY month\n",
    "            ''')\n",
    "            \n",
    "            # 用户行为分析视图\n",
    "            cursor.execute('''\n",
    "            CREATE VIEW IF NOT EXISTS vw_user_behavior AS\n",
    "            SELECT \n",
    "                bl.customer_id,\n",
    "                bl.behavior_type,\n",
    "                COUNT(bl.log_id) as behavior_count,\n",
    "                COUNT(DISTINCT bl.product_id) as unique_products,\n",
    "                COUNT(DISTINCT DATE(bl.timestamp)) as active_days\n",
    "            FROM behavior_logs bl\n",
    "            GROUP BY bl.customer_id, bl.behavior_type\n",
    "            ''')\n",
    "            \n",
    "            self.connection.commit()\n",
    "            logger.info(\"[INFO] 数据仓库视图创建完成\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ERROR] 创建数据仓库视图失败: {str(e)}\")\n",
    "    \n",
    "    def generate_execution_report(self, results: Dict):\n",
    "        \"\"\"生成执行报告\"\"\"\n",
    "        try:\n",
    "            report_data = {\n",
    "                'execution_summary': {\n",
    "                    'execution_id': results['execution_id'],\n",
    "                    'start_time': results['start_time'],\n",
    "                    'end_time': results['end_time'],\n",
    "                    'total_duration': results['total_duration'],\n",
    "                    'status': results['status'],\n",
    "                    'tables_processed': results.get('tables_processed', 0)\n",
    "                },\n",
    "                'stage_details': results.get('stages', {}),\n",
    "                'quality_metrics': self.data_quality_metrics,\n",
    "                'quality_report': results.get('quality_report', {}),\n",
    "                'generated_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # 保存JSON报告\n",
    "            with open('etl_execution_report.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "            \n",
    "            # 生成文本报告\n",
    "            self._generate_text_report(report_data)\n",
    "            \n",
    "            logger.info(f\"[INFO] 执行报告已生成: etl_execution_report.json\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ERROR] 生成执行报告失败: {str(e)}\")\n",
    "    \n",
    "    def _generate_text_report(self, report_data: Dict):\n",
    "        \"\"\"生成文本格式报告\"\"\"\n",
    "        report_lines = []\n",
    "        report_lines.append(\"=\"*80)\n",
    "        report_lines.append(\" ETL管道执行报告\")\n",
    "        report_lines.append(f\"生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report_lines.append(\"=\"*80)\n",
    "        \n",
    "        # 执行摘要\n",
    "        summary = report_data['execution_summary']\n",
    "        report_lines.append(f\"\\n [SUMMARY] 执行摘要\")\n",
    "        report_lines.append(f\"-\"*40)\n",
    "        report_lines.append(f\"执行ID: {summary['execution_id']}\")\n",
    "        report_lines.append(f\"状态: {summary['status']}\")\n",
    "        report_lines.append(f\"开始时间: {summary['start_time']}\")\n",
    "        report_lines.append(f\"结束时间: {summary['end_time']}\")\n",
    "        report_lines.append(f\"总耗时: {summary['total_duration']:.2f} 秒\")\n",
    "        report_lines.append(f\"处理表数: {summary['tables_processed']}\")\n",
    "        \n",
    "        # 阶段详情\n",
    "        stages = report_data['stage_details']\n",
    "        report_lines.append(f\"\\n [STAGES] 阶段详情\")\n",
    "        report_lines.append(f\"-\"*40)\n",
    "        \n",
    "        for stage_name, stage_info in stages.items():\n",
    "            duration = stage_info.get('duration', 0)\n",
    "            status = stage_info.get('status', 'unknown')\n",
    "            report_lines.append(f\"{stage_name:15s}: {status:10s} | 耗时: {duration:.2f}秒\")\n",
    "        \n",
    "        # 质量报告\n",
    "        quality = report_data.get('quality_report', {})\n",
    "        if quality:\n",
    "            report_lines.append(f\"\\n [QUALITY] 数据质量报告\")\n",
    "            report_lines.append(f\"-\"*40)\n",
    "            report_lines.append(f\"总体质量评分: {quality.get('overall_score', 0):.1f}/100\")\n",
    "            report_lines.append(f\"发现问题总数: {quality.get('issues_found', 0)}\")\n",
    "            \n",
    "            # 各表质量\n",
    "            tables = quality.get('tables', {})\n",
    "            if tables:\n",
    "                report_lines.append(f\"\\n 各表质量:\")\n",
    "                for table_name, table_info in tables.items():\n",
    "                    score = table_info.get('quality_score', 0)\n",
    "                    issues = table_info.get('issues_count', 0)\n",
    "                    report_lines.append(f\"  {table_name:15s}: {score:5.1f}/100 | 问题数: {issues}\")\n",
    "        \n",
    "        # 保存文本报告\n",
    "        with open('etl_report.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(report_lines))\n",
    "        \n",
    "        # 打印报告\n",
    "        print('\\n'.join(report_lines))\n",
    "    \n",
    "    def generate_dashboard(self, results: Dict, quality_report: Dict):\n",
    "        \"\"\"生成可视化仪表板\"\"\"\n",
    "        try:\n",
    "            # 创建仪表板\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=3,\n",
    "                subplot_titles=('ETL阶段耗时', '数据质量评分', '表级质量分布',\n",
    "                              '执行历史趋势', '问题类型分布', '数据量变化'),\n",
    "                specs=[[{'type': 'bar'}, {'type': 'gauge'}, {'type': 'bar'}],\n",
    "                       [{'type': 'line'}, {'type': 'pie'}, {'type': 'bar'}]]\n",
    "            )\n",
    "            \n",
    "            # 1. ETL阶段耗时柱状图\n",
    "            stages = results.get('stages', {})\n",
    "            stage_names = list(stages.keys())\n",
    "            stage_durations = [s.get('duration', 0) for s in stages.values()]\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(x=stage_names, y=stage_durations, name='阶段耗时', marker_color='skyblue'),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # 2. 数据质量评分仪表盘\n",
    "            quality_score = quality_report.get('overall_score', 0)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Indicator(\n",
    "                    mode=\"gauge+number\",\n",
    "                    value=quality_score,\n",
    "                    title={'text': \"数据质量评分\"},\n",
    "                    domain={'row': 0, 'column': 1},\n",
    "                    gauge={\n",
    "                        'axis': {'range': [0, 100]},\n",
    "                        'bar': {'color': \"darkblue\"},\n",
    "                        'steps': [\n",
    "                            {'range': [0, 60], 'color': \"red\"},\n",
    "                            {'range': [60, 80], 'color': \"yellow\"},\n",
    "                            {'range': [80, 100], 'color': \"green\"}\n",
    "                        ]\n",
    "                    }\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # 3. 表级质量分布\n",
    "            tables = quality_report.get('tables', {})\n",
    "            table_names = list(tables.keys())\n",
    "            table_scores = [t.get('quality_score', 0) for t in tables.values()]\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(x=table_names, y=table_scores, name='表质量', marker_color='lightgreen'),\n",
    "                row=1, col=3\n",
    "            )\n",
    "            \n",
    "            # 4. 执行历史趋势（模拟数据）\n",
    "            if self.execution_history:\n",
    "                history_dates = [h['timestamp'] for h in self.execution_history[-5:]]\n",
    "                history_scores = [h.get('quality_score', 0) for h in self.execution_history[-5:]]\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=history_dates, y=history_scores, mode='lines+markers', name='质量趋势'),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "            \n",
    "            # 5. 问题类型分布\n",
    "            issues = quality_report.get('detailed_issues', [])\n",
    "            if issues:\n",
    "                issue_types = [i['type'] for i in issues]\n",
    "                issue_counts = pd.Series(issue_types).value_counts()\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Pie(labels=issue_counts.index.tolist(), values=issue_counts.values.tolist(), name='问题分布'),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "            \n",
    "            # 6. 数据量变化\n",
    "            if 'transformation' in self.data_quality_metrics:\n",
    "                transform_stats = self.data_quality_metrics['transformation']\n",
    "                table_names = list(transform_stats.keys())\n",
    "                before_rows = [s['before']['rows'] for s in transform_stats.values()]\n",
    "                after_rows = [s['after']['rows'] for s in transform_stats.values()]\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Bar(x=table_names, y=before_rows, name='清洗前', marker_color='red'),\n",
    "                    row=2, col=3\n",
    "                )\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Bar(x=table_names, y=after_rows, name='清洗后', marker_color='green'),\n",
    "                    row=2, col=3\n",
    "                )\n",
    "            \n",
    "            # 更新布局\n",
    "            fig.update_layout(\n",
    "                height=800,\n",
    "                showlegend=True,\n",
    "                title_text=\"ETL管道监控仪表板\",\n",
    "                title_font_size=20\n",
    "            )\n",
    "            \n",
    "            # 保存为HTML\n",
    "            fig.write_html(\"etl_dashboard.html\")\n",
    "            \n",
    "            # 在Jupyter中显示\n",
    "            fig.show()\n",
    "            \n",
    "            logger.info(f\"[INFO] 仪表板已生成: etl_dashboard.html\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ERROR] 生成仪表板失败: {str(e)}\")\n",
    "    \n",
    "    def close_connection(self):\n",
    "        \"\"\"关闭数据库连接\"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            logger.info(\"[INFO] 数据库连接已关闭\")\n",
    "\n",
    "logger.info(\"[OK] ETL管道架构类定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7135fbce-df0f-4c1c-8ed4-77d6013f085c",
   "metadata": {},
   "source": [
    "# 3.数据质量监控模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e5b80d-e778-4c00-a4ba-c93af9d26ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:39:57,626 - ETL_Pipeline - INFO - [OK] 数据质量监控模块定义完成\n"
     ]
    }
   ],
   "source": [
    "class DataQualityMonitor:\n",
    "    \"\"\"\n",
    "    数据质量监控模块\n",
    "    提供实时的数据质量检查和告警功能\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = 'ecommerce_clean.db'):\n",
    "        \"\"\"\n",
    "        初始化数据质量监控器\n",
    "        \n",
    "        Args:\n",
    "            db_path: 数据库路径\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.connection = None\n",
    "        self.monitoring_rules = self._load_monitoring_rules()\n",
    "        self.alert_history = []\n",
    "        self.quality_metrics_history = []\n",
    "        \n",
    "        logger.info(f\"[INFO] 数据质量监控器初始化 - 数据库: {db_path}\")\n",
    "    \n",
    "    def _load_monitoring_rules(self) -> Dict:\n",
    "        \"\"\"加载监控规则\"\"\"\n",
    "        return {\n",
    "            'missing_values': {\n",
    "                'threshold': 0.05,  # 5%缺失值阈值\n",
    "                'severity': 'warning',\n",
    "                'message': '缺失值超过阈值'\n",
    "            },\n",
    "            'duplicate_rows': {\n",
    "                'threshold': 0.01,  # 1%重复行阈值\n",
    "                'severity': 'warning',\n",
    "                'message': '重复行超过阈值'\n",
    "            },\n",
    "            'negative_values': {\n",
    "                'threshold': 0,\n",
    "                'severity': 'error',\n",
    "                'message': '发现负值'\n",
    "            },\n",
    "            'outliers': {\n",
    "                'threshold': 3,  # 3倍标准差\n",
    "                'severity': 'info',\n",
    "                'message': '发现异常值'\n",
    "            },\n",
    "            'referential_integrity': {\n",
    "                'threshold': 0,\n",
    "                'severity': 'error',\n",
    "                'message': '引用完整性错误'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def connect_to_db(self):\n",
    "        \"\"\"连接到数据库\"\"\"\n",
    "        try:\n",
    "            self.connection = sqlite3.connect(self.db_path)\n",
    "            logger.info(f\"[INFO] 监控器成功连接到数据库: {self.db_path}\")\n",
    "        except Error as e:\n",
    "            logger.error(f\"[ERROR] 数据库连接失败: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def run_comprehensive_monitoring(self) -> Dict:\n",
    "        \"\"\"\n",
    "        运行全面的数据质量监控\n",
    "        \n",
    "        Returns:\n",
    "            Dict: 监控结果\n",
    "        \"\"\"\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\" [MONITOR] 开始全面数据质量监控\")\n",
    "        logger.info(\"=\"*80)\n",
    "        \n",
    "        monitoring_start = datetime.now()\n",
    "        results = {\n",
    "            'timestamp': monitoring_start.isoformat(),\n",
    "            'tables_monitored': [],\n",
    "            'alerts': [],\n",
    "            'metrics': {},\n",
    "            'summary': {\n",
    "                'total_alerts': 0,\n",
    "                'critical_alerts': 0,\n",
    "                'warning_alerts': 0,\n",
    "                'info_alerts': 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            if not self.connection:\n",
    "                self.connect_to_db()\n",
    "            \n",
    "            # 获取所有表\n",
    "            cursor = self.connection.cursor()\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "            tables = [row[0] for row in cursor.fetchall()]\n",
    "            \n",
    "            # 监控每个表\n",
    "            for table_name in tables:\n",
    "                logger.info(f\"[INFO] 监控表: {table_name}\")\n",
    "                table_results = self._monitor_table(table_name)\n",
    "                results['tables_monitored'].append(table_name)\n",
    "                results['metrics'][table_name] = table_results['metrics']\n",
    "                results['alerts'].extend(table_results['alerts'])\n",
    "            \n",
    "            # 运行跨表监控\n",
    "            cross_table_results = self._monitor_cross_table_integrity()\n",
    "            results['alerts'].extend(cross_table_results['alerts'])\n",
    "            \n",
    "            # 运行业务规则监控\n",
    "            business_rule_results = self._monitor_business_rules()\n",
    "            results['alerts'].extend(business_rule_results['alerts'])\n",
    "            \n",
    "            # 更新摘要统计\n",
    "            for alert in results['alerts']:\n",
    "                results['summary']['total_alerts'] += 1\n",
    "                if alert['severity'] == 'error':\n",
    "                    results['summary']['critical_alerts'] += 1\n",
    "                elif alert['severity'] == 'warning':\n",
    "                    results['summary']['warning_alerts'] += 1\n",
    "                elif alert['severity'] == 'info':\n",
    "                    results['summary']['info_alerts'] += 1\n",
    "            \n",
    "            # 记录监控历史\n",
    "            monitoring_end = datetime.now()\n",
    "            monitoring_record = {\n",
    "                'timestamp': monitoring_start,\n",
    "                'duration': (monitoring_end - monitoring_start).total_seconds(),\n",
    "                'tables_monitored': len(tables),\n",
    "                'total_alerts': results['summary']['total_alerts'],\n",
    "                'critical_alerts': results['summary']['critical_alerts']\n",
    "            }\n",
    "            self.quality_metrics_history.append(monitoring_record)\n",
    "            \n",
    "            # 生成监控报告\n",
    "            self._generate_monitoring_report(results)\n",
    "            \n",
    "            logger.info(f\"[INFO] 监控完成 - 发现告警: {results['summary']['total_alerts']} 个\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ERROR] 监控运行失败: {str(e)}\")\n",
    "            results['error'] = str(e)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _monitor_table(self, table_name: str) -> Dict:\n",
    "        \"\"\"监控单个表\"\"\"\n",
    "        results = {\n",
    "            'table_name': table_name,\n",
    "            'metrics': {},\n",
    "            'alerts': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 读取表数据\n",
    "            df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", self.connection)\n",
    "            \n",
    "            # 计算基本指标\n",
    "            metrics = {\n",
    "                'row_count': len(df),\n",
    "                'column_count': len(df.columns),\n",
    "                'missing_values': df.isnull().sum().sum(),\n",
    "                'missing_percentage': (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100,\n",
    "                'duplicate_rows': df.duplicated().sum(),\n",
    "                'duplicate_percentage': (df.duplicated().sum() / len(df)) * 100 if len(df) > 0 else 0\n",
    "            }\n",
    "            \n",
    "            results['metrics'] = metrics\n",
    "            \n",
    "            # 检查缺失值\n",
    "            missing_threshold = self.monitoring_rules['missing_values']['threshold']\n",
    "            if metrics['missing_percentage'] > missing_threshold * 100:\n",
    "                alert = {\n",
    "                    'table': table_name,\n",
    "                    'type': 'missing_values',\n",
    "                    'severity': self.monitoring_rules['missing_values']['severity'],\n",
    "                    'message': f\"{table_name}: 缺失值比例 {metrics['missing_percentage']:.2f}% 超过阈值 {missing_threshold*100}%\",\n",
    "                    'value': metrics['missing_percentage'],\n",
    "                    'threshold': missing_threshold * 100\n",
    "                }\n",
    "                results['alerts'].append(alert)\n",
    "                self._record_alert(alert)\n",
    "            \n",
    "            # 检查重复行\n",
    "            duplicate_threshold = self.monitoring_rules['duplicate_rows']['threshold']\n",
    "            if metrics['duplicate_percentage'] > duplicate_threshold * 100:\n",
    "                alert = {\n",
    "                    'table': table_name,\n",
    "                    'type': 'duplicate_rows',\n",
    "                    'severity': self.monitoring_rules['duplicate_rows']['severity'],\n",
    "                    'message': f\"{table_name}: 重复行比例 {metrics['duplicate_percentage']:.2f}% 超过阈值 {duplicate_threshold*100}%\",\n",
    "                    'value': metrics['duplicate_percentage'],\n",
    "                    'threshold': duplicate_threshold * 100\n",
    "                }\n",
    "                results['alerts'].append(alert)\n",
    "                self._record_alert(alert)\n",
    "            \n",
    "            # 检查数值列的负值\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            for col in numeric_cols:\n",
    "                if (df[col] < 0).any():\n",
    "                    negative_count = (df[col] < 0).sum()\n",
    "                    alert = {\n",
    "                        'table': table_name,\n",
    "                        'type': 'negative_values',\n",
    "                        'severity': self.monitoring_rules['negative_values']['severity'],\n",
    "                        'message': f\"{table_name}.{col}: 发现 {negative_count} 个负值\",\n",
    "                        'value': negative_count,\n",
    "                        'threshold': 0\n",
    "                    }\n",
    "                    results['alerts'].append(alert)\n",
    "                    self._record_alert(alert)\n",
    "            \n",
    "            # 检查异常值（针对数值列）\n",
    "            for col in numeric_cols:\n",
    "                if df[col].dtype in [np.float64, np.int64]:\n",
    "                    mean = df[col].mean()\n",
    "                    std = df[col].std()\n",
    "                    if std > 0:\n",
    "                        outliers = df[abs(df[col] - mean) > self.monitoring_rules['outliers']['threshold'] * std]\n",
    "                        if len(outliers) > 0:\n",
    "                            outlier_percentage = (len(outliers) / len(df)) * 100\n",
    "                            if outlier_percentage > 1:  # 超过1%的异常值才告警\n",
    "                                alert = {\n",
    "                                    'table': table_name,\n",
    "                                    'type': 'outliers',\n",
    "                                    'severity': self.monitoring_rules['outliers']['severity'],\n",
    "                                    'message': f\"{table_name}.{col}: 发现 {len(outliers)} 个异常值 ({outlier_percentage:.2f}%)\",\n",
    "                                    'value': outlier_percentage,\n",
    "                                    'threshold': 1\n",
    "                                }\n",
    "                                results['alerts'].append(alert)\n",
    "                                self._record_alert(alert)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_alert = {\n",
    "                'table': table_name,\n",
    "                'type': 'monitoring_error',\n",
    "                'severity': 'error',\n",
    "                'message': f\"监控表 {table_name} 时出错: {str(e)}\"\n",
    "            }\n",
    "            results['alerts'].append(error_alert)\n",
    "            self._record_alert(error_alert)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _monitor_cross_table_integrity(self) -> Dict:\n",
    "        \"\"\"监控跨表引用完整性\"\"\"\n",
    "        results = {\n",
    "            'type': 'cross_table_integrity',\n",
    "            'alerts': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 检查订单-客户引用完整性\n",
    "            cursor = self.connection.cursor()\n",
    "            \n",
    "            cursor.execute('''\n",
    "            SELECT COUNT(*) FROM orders o \n",
    "            LEFT JOIN customers c ON o.customer_id = c.customer_id \n",
    "            WHERE c.customer_id IS NULL\n",
    "            ''')\n",
    "            missing_customers = cursor.fetchone()[0]\n",
    "            \n",
    "            if missing_customers > 0:\n",
    "                alert = {\n",
    "                    'type': 'referential_integrity',\n",
    "                    'severity': self.monitoring_rules['referential_integrity']['severity'],\n",
    "                    'message': f\"订单表引用了 {missing_customers} 个不存在的客户ID\",\n",
    "                    'value': missing_customers,\n",
    "                    'threshold': 0\n",
    "                }\n",
    "                results['alerts'].append(alert)\n",
    "                self._record_alert(alert)\n",
    "            \n",
    "            # 检查订单-产品引用完整性\n",
    "            cursor.execute('''\n",
    "            SELECT COUNT(*) FROM orders o \n",
    "            LEFT JOIN products p ON o.product_id = p.product_id \n",
    "            WHERE p.product_id IS NULL\n",
    "            ''')\n",
    "            missing_products = cursor.fetchone()[0]\n",
    "            \n",
    "            if missing_products > 0:\n",
    "                alert = {\n",
    "                    'type': 'referential_integrity',\n",
    "                    'severity': self.monitoring_rules['referential_integrity']['severity'],\n",
    "                    'message': f\"订单表引用了 {missing_products} 个不存在的产品ID\",\n",
    "                    'value': missing_products,\n",
    "                    'threshold': 0\n",
    "                }\n",
    "                results['alerts'].append(alert)\n",
    "                self._record_alert(alert)\n",
    "            \n",
    "            # 检查行为日志引用完整性\n",
    "            cursor.execute('''\n",
    "            SELECT COUNT(*) FROM behavior_logs bl \n",
    "            LEFT JOIN customers c ON bl.customer_id = c.customer_id \n",
    "            WHERE c.customer_id IS NULL\n",
    "            ''')\n",
    "            missing_behavior_customers = cursor.fetchone()[0]\n",
    "            \n",
    "            if missing_behavior_customers > 0:\n",
    "                alert = {\n",
    "                    'type': 'referential_integrity',\n",
    "                    'severity': self.monitoring_rules['referential_integrity']['severity'],\n",
    "                    'message': f\"行为日志表引用了 {missing_behavior_customers} 个不存在的客户ID\",\n",
    "                    'value': missing_behavior_customers,\n",
    "                    'threshold': 0\n",
    "                }\n",
    "                results['alerts'].append(alert)\n",
    "                self._record_alert(alert)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_alert = {\n",
    "                'type': 'integrity_check_error',\n",
    "                'severity': 'error',\n",
    "                'message': f\"检查引用完整性时出错: {str(e)}\"\n",
    "            }\n",
    "            results['alerts'].append(error_alert)\n",
    "            self._record_alert(error_alert)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _monitor_business_rules(self) -> Dict:\n",
    "        \"\"\"监控业务规则\"\"\"\n",
    "        results = {\n",
    "            'type': 'business_rules',\n",
    "            'alerts': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 检查订单金额计算正确性\n",
    "            cursor = self.connection.cursor()\n",
    "            \n",
    "            cursor.execute('''\n",
    "            SELECT COUNT(*) FROM orders \n",
    "            WHERE ABS(amount - (unit_price * quantity)) > 0.01\n",
    "            ''')\n",
    "            amount_errors = cursor.fetchone()[0]\n",
    "            \n",
    "            if amount_errors > 0:\n",
    "                alert = {\n",
    "                    'type': 'business_rule',\n",
    "                    'severity': 'error',\n",
    "                    'message': f\"发现 {amount_errors} 个订单金额计算错误\",\n",
    "                    'value': amount_errors,\n",
    "                    'threshold': 0\n",
    "                }\n",
    "                results['alerts'].append(alert)\n",
    "                self._record_alert(alert)\n",
    "            \n",
    "            # 检查产品价格合理性\n",
    "            cursor.execute('''\n",
    "            SELECT COUNT(*) FROM products WHERE price <= 0\n",
    "            ''')\n",
    "            invalid_prices = cursor.fetchone()[0]\n",
    "            \n",
    "            if invalid_prices > 0:\n",
    "                alert = {\n",
    "                    'type': 'business_rule',\n",
    "                    'severity': 'error',\n",
    "                    'message': f\"发现 {invalid_prices} 个无效的产品价格\",\n",
    "                    'value': invalid_prices,\n",
    "                    'threshold': 0\n",
    "                }\n",
    "                results['alerts'].append(alert)\n",
    "                self._record_alert(alert)\n",
    "            \n",
    "            # 检查库存合理性\n",
    "            cursor.execute('''\n",
    "            SELECT COUNT(*) FROM products WHERE stock_quantity < 0\n",
    "            ''')\n",
    "            negative_stock = cursor.fetchone()[0]\n",
    "            \n",
    "            if negative_stock > 0:\n",
    "                alert = {\n",
    "                    'type': 'business_rule',\n",
    "                    'severity': 'error',\n",
    "                    'message': f\"发现 {negative_stock} 个负库存产品\",\n",
    "                    'value': negative_stock,\n",
    "                    'threshold': 0\n",
    "                }\n",
    "                results['alerts'].append(alert)\n",
    "                self._record_alert(alert)\n",
    "            \n",
    "            # 检查订单日期合理性\n",
    "            cursor.execute('''\n",
    "            SELECT COUNT(*) FROM orders WHERE order_date > date('now')\n",
    "            ''')\n",
    "            future_orders = cursor.fetchone()[0]\n",
    "            \n",
    "            if future_orders > 0:\n",
    "                alert = {\n",
    "                    'type': 'business_rule',\n",
    "                    'severity': 'warning',\n",
    "                    'message': f\"发现 {future_orders} 个未来日期的订单\",\n",
    "                    'value': future_orders,\n",
    "                    'threshold': 0\n",
    "                }\n",
    "                results['alerts'].append(alert)\n",
    "                self._record_alert(alert)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_alert = {\n",
    "                'type': 'business_rule_error',\n",
    "                'severity': 'error',\n",
    "                'message': f\"检查业务规则时出错: {str(e)}\"\n",
    "            }\n",
    "            results['alerts'].append(error_alert)\n",
    "            self._record_alert(error_alert)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _record_alert(self, alert: Dict):\n",
    "        \"\"\"记录告警\"\"\"\n",
    "        alert['timestamp'] = datetime.now().isoformat()\n",
    "        self.alert_history.append(alert)\n",
    "        \n",
    "        # 根据严重程度打印不同颜色的日志\n",
    "        severity = alert.get('severity', 'info')\n",
    "        message = alert.get('message', '')\n",
    "        \n",
    "        if severity == 'error':\n",
    "            logger.error(f\"[ERROR] {message}\")\n",
    "        elif severity == 'warning':\n",
    "            logger.warning(f\"[WARNING] {message}\")\n",
    "        else:\n",
    "            logger.info(f\"[INFO] {message}\")\n",
    "    \n",
    "    def _generate_monitoring_report(self, results: Dict):\n",
    "        \"\"\"生成监控报告\"\"\"\n",
    "        try:\n",
    "            report_data = {\n",
    "                'monitoring_summary': results['summary'],\n",
    "                'alerts': results['alerts'],\n",
    "                'metrics': results['metrics'],\n",
    "                'timestamp': results['timestamp'],\n",
    "                'tables_monitored': results['tables_monitored']\n",
    "            }\n",
    "            \n",
    "            # 保存JSON报告\n",
    "            with open('data_quality_monitoring_report.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "            \n",
    "            # 生成文本报告\n",
    "            self._generate_monitoring_text_report(report_data)\n",
    "            \n",
    "            # 生成可视化报告\n",
    "            self._generate_monitoring_visualization(report_data)\n",
    "            \n",
    "            logger.info(f\"[INFO] 监控报告已生成: data_quality_monitoring_report.json\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ERROR] 生成监控报告失败: {str(e)}\")\n",
    "    \n",
    "    def _generate_monitoring_text_report(self, report_data: Dict):\n",
    "        \"\"\"生成文本格式监控报告\"\"\"\n",
    "        report_lines = []\n",
    "        report_lines.append(\"=\"*80)\n",
    "        report_lines.append(\" [REPORT] 数据质量监控报告\")\n",
    "        report_lines.append(f\"生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report_lines.append(\"=\"*80)\n",
    "        \n",
    "        # 摘要\n",
    "        summary = report_data['monitoring_summary']\n",
    "        report_lines.append(f\"\\n [SUMMARY] 监控摘要\")\n",
    "        report_lines.append(f\"-\"*40)\n",
    "        report_lines.append(f\"监控表数: {len(report_data['tables_monitored'])}\")\n",
    "        report_lines.append(f\"总告警数: {summary['total_alerts']}\")\n",
    "        report_lines.append(f\"严重告警: {summary['critical_alerts']}\")\n",
    "        report_lines.append(f\"警告告警: {summary['warning_alerts']}\")\n",
    "        report_lines.append(f\"信息告警: {summary['info_alerts']}\")\n",
    "        \n",
    "        # 告警详情\n",
    "        alerts = report_data['alerts']\n",
    "        if alerts:\n",
    "            report_lines.append(f\"\\n [ALERTS] 告警详情\")\n",
    "            report_lines.append(f\"-\"*40)\n",
    "            \n",
    "            # 按严重程度分组\n",
    "            error_alerts = [a for a in alerts if a['severity'] == 'error']\n",
    "            warning_alerts = [a for a in alerts if a['severity'] == 'warning']\n",
    "            info_alerts = [a for a in alerts if a['severity'] == 'info']\n",
    "            \n",
    "            if error_alerts:\n",
    "                report_lines.append(f\"\\n[严重告警] ({len(error_alerts)} 个):\")\n",
    "                for alert in error_alerts[:5]:  # 只显示前5个\n",
    "                    report_lines.append(f\"  • {alert['message']}\")\n",
    "                if len(error_alerts) > 5:\n",
    "                    report_lines.append(f\"  ... 还有 {len(error_alerts) - 5} 个严重告警\")\n",
    "            \n",
    "            if warning_alerts:\n",
    "                report_lines.append(f\"\\n[警告告警] ({len(warning_alerts)} 个):\")\n",
    "                for alert in warning_alerts[:5]:\n",
    "                    report_lines.append(f\"  • {alert['message']}\")\n",
    "                if len(warning_alerts) > 5:\n",
    "                    report_lines.append(f\"  ... 还有 {len(warning_alerts) - 5} 个警告告警\")\n",
    "            \n",
    "            if info_alerts:\n",
    "                report_lines.append(f\"\\n[信息告警] ({len(info_alerts)} 个):\")\n",
    "                for alert in info_alerts[:3]:\n",
    "                    report_lines.append(f\"  • {alert['message']}\")\n",
    "                if len(info_alerts) > 3:\n",
    "                    report_lines.append(f\"  ... 还有 {len(info_alerts) - 3} 个信息告警\")\n",
    "        \n",
    "        else:\n",
    "            report_lines.append(f\"\\n [OK] 未发现任何告警，数据质量良好!\")\n",
    "        \n",
    "        # 保存文本报告\n",
    "        with open('data_quality_report.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(report_lines))\n",
    "        \n",
    "        # 打印报告\n",
    "        print('\\n'.join(report_lines))\n",
    "    \n",
    "    def _generate_monitoring_visualization(self, report_data: Dict):\n",
    "        \"\"\"生成监控可视化图表\"\"\"\n",
    "        try:\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=2,\n",
    "                subplot_titles=('告警严重程度分布', '告警类型分布', \n",
    "                              '监控历史趋势', '表级问题数量'),\n",
    "                specs=[[{'type': 'pie'}, {'type': 'bar'}],\n",
    "                       [{'type': 'line'}, {'type': 'bar'}]]\n",
    "            )\n",
    "            \n",
    "            # 1. 告警严重程度分布饼图\n",
    "            summary = report_data['monitoring_summary']\n",
    "            severity_counts = [\n",
    "                summary['critical_alerts'],\n",
    "                summary['warning_alerts'],\n",
    "                summary['info_alerts']\n",
    "            ]\n",
    "            severity_labels = ['严重', '警告', '信息']\n",
    "            severity_colors = ['red', 'yellow', 'blue']\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Pie(labels=severity_labels, values=severity_counts, \n",
    "                      marker_colors=severity_colors, name='告警严重程度'),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # 2. 告警类型分布柱状图\n",
    "            alerts = report_data['alerts']\n",
    "            if alerts:\n",
    "                alert_types = [a['type'] for a in alerts]\n",
    "                type_counts = pd.Series(alert_types).value_counts()\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Bar(x=type_counts.index.tolist(), y=type_counts.values.tolist(),\n",
    "                          name='告警类型', marker_color='lightcoral'),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "            \n",
    "            # 3. 监控历史趋势线图\n",
    "            if self.quality_metrics_history:\n",
    "                history_dates = [h['timestamp'] for h in self.quality_metrics_history]\n",
    "                history_alerts = [h['total_alerts'] for h in self.quality_metrics_history]\n",
    "                history_critical = [h['critical_alerts'] for h in self.quality_metrics_history]\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=history_dates, y=history_alerts, mode='lines+markers',\n",
    "                              name='总告警数', line=dict(color='blue')),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=history_dates, y=history_critical, mode='lines+markers',\n",
    "                              name='严重告警数', line=dict(color='red')),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "            \n",
    "            # 4. 表级问题数量\n",
    "            metrics = report_data.get('metrics', {})\n",
    "            if metrics:\n",
    "                table_names = list(metrics.keys())\n",
    "                table_problems = []\n",
    "                \n",
    "                for table in table_names:\n",
    "                    table_metrics = metrics[table]\n",
    "                    # 计算问题分数\n",
    "                    problem_score = (table_metrics.get('missing_percentage', 0) / 5 + \n",
    "                                   table_metrics.get('duplicate_percentage', 0))\n",
    "                    table_problems.append(problem_score)\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Bar(x=table_names, y=table_problems, name='表问题分数',\n",
    "                          marker_color='lightgreen'),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "            \n",
    "            # 更新布局\n",
    "            fig.update_layout(\n",
    "                height=700,\n",
    "                showlegend=True,\n",
    "                title_text=\"数据质量监控仪表板\",\n",
    "                title_font_size=20\n",
    "            )\n",
    "            \n",
    "            # 保存为HTML\n",
    "            fig.write_html(\"data_quality_dashboard.html\")\n",
    "            \n",
    "            # 在Jupyter中显示\n",
    "            fig.show()\n",
    "            \n",
    "            logger.info(f\"[INFO] 监控仪表板已生成: data_quality_dashboard.html\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ERROR] 生成监控可视化失败: {str(e)}\")\n",
    "    \n",
    "    def setup_real_time_monitoring(self, interval_minutes: int = 5):\n",
    "        \"\"\"设置实时监控（模拟）\"\"\"\n",
    "        logger.info(f\"[INFO] 设置实时监控，每 {interval_minutes} 分钟运行一次\")\n",
    "        \n",
    "        def monitoring_job():\n",
    "            logger.info(\"[TIME] 执行定时监控任务...\")\n",
    "            results = self.run_comprehensive_monitoring()\n",
    "            \n",
    "            # 如果有严重告警，发送通知\n",
    "            if results['summary']['critical_alerts'] > 0:\n",
    "                self._send_alert_notification(results)\n",
    "        \n",
    "        # 在实际应用中，这里会使用定时任务调度器\n",
    "        # 这里只是模拟\n",
    "        logger.info(\"[INFO] 实时监控已设置（模拟模式）\")\n",
    "        logger.info(f\"[INFO] 下次监控将在 {interval_minutes} 分钟后运行\")\n",
    "    \n",
    "    def _send_alert_notification(self, results: Dict):\n",
    "        \"\"\"发送告警通知（模拟）\"\"\"\n",
    "        critical_alerts = results['summary']['critical_alerts']\n",
    "        \n",
    "        notification = f\"\"\"\n",
    "        [ALERT] 数据质量告警通知\n",
    "        ====================\n",
    "        时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        发现 {critical_alerts} 个严重告警！\n",
    "        \n",
    "        请立即检查数据质量监控报告：\n",
    "        - data_quality_monitoring_report.json\n",
    "        - data_quality_report.txt\n",
    "        - data_quality_dashboard.html\n",
    "        \n",
    "        需要立即处理的问题：\n",
    "        \"\"\"\n",
    "        \n",
    "        # 获取前3个严重告警\n",
    "        critical_alerts_list = [a for a in results['alerts'] if a['severity'] == 'error'][:3]\n",
    "        for alert in critical_alerts_list:\n",
    "            notification += f\"\\n• {alert['message']}\"\n",
    "        \n",
    "        # 在实际应用中，这里会发送邮件、Slack消息等\n",
    "        logger.warning(\"[WARNING] 发送告警通知（模拟）:\")\n",
    "        print(notification)\n",
    "    \n",
    "    def get_quality_score(self) -> float:\n",
    "        \"\"\"计算数据质量综合评分\"\"\"\n",
    "        try:\n",
    "            if not self.connection:\n",
    "                self.connect_to_db()\n",
    "            \n",
    "            # 运行监控获取最新数据\n",
    "            results = self.run_comprehensive_monitoring()\n",
    "            \n",
    "            # 计算质量评分\n",
    "            total_alerts = results['summary']['total_alerts']\n",
    "            critical_alerts = results['summary']['critical_alerts']\n",
    "            \n",
    "            # 基础分100分\n",
    "            score = 100\n",
    "            \n",
    "            # 扣分规则\n",
    "            score -= critical_alerts * 10  # 每个严重告警扣10分\n",
    "            score -= (total_alerts - critical_alerts) * 2  # 每个非严重告警扣2分\n",
    "            \n",
    "            # 确保分数在0-100之间\n",
    "            score = max(0, min(100, score))\n",
    "            \n",
    "            return score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ERROR] 计算质量评分失败: {str(e)}\")\n",
    "            return 0\n",
    "    \n",
    "    def close_connection(self):\n",
    "        \"\"\"关闭数据库连接\"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            logger.info(\"[INFO] 监控器数据库连接已关闭\")\n",
    "\n",
    "logger.info(\"[OK] 数据质量监控模块定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2da90d-9861-4a07-834e-97e5157489ad",
   "metadata": {},
   "source": [
    "# 4.ETL管道自动化运行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a4cd0e-0ec6-4f2e-8aa3-daf01aa927cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:41:12,125 - ETL_Pipeline - INFO - [OK] ETL编排器类定义完成\n"
     ]
    }
   ],
   "source": [
    "class ETLOrchestrator:\n",
    "    \"\"\"\n",
    "    ETL管道自动化编排器\n",
    "    管理ETL管道的调度、执行和监控\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"初始化编排器\"\"\"\n",
    "        self.etl_pipeline = None\n",
    "        self.quality_monitor = None\n",
    "        self.schedule_config = {\n",
    "            'etl_schedule': 'daily',  # daily, weekly, manual\n",
    "            'monitoring_interval': 60,  # 监控间隔（分钟）\n",
    "            'retry_count': 3,\n",
    "            'notify_on_failure': True\n",
    "        }\n",
    "        self.execution_tracker = []\n",
    "        \n",
    "        logger.info(\"[INFO] ETL编排器初始化完成\")\n",
    "    \n",
    "    def initialize_components(self, source_dir: str = '.', db_path: str = 'ecommerce_clean.db'):\n",
    "        \"\"\"初始化组件\"\"\"\n",
    "        logger.info(\"[INFO] 初始化ETL组件...\")\n",
    "        \n",
    "        # 初始化ETL管道\n",
    "        self.etl_pipeline = EcommerceETLPipeline(source_dir=source_dir, db_path=db_path)\n",
    "        logger.info(\"[OK] ETL管道初始化完成\")\n",
    "        \n",
    "        # 初始化数据质量监控器\n",
    "        self.quality_monitor = DataQualityMonitor(db_path=db_path)\n",
    "        logger.info(\"[OK] 数据质量监控器初始化完成\")\n",
    "        \n",
    "        # 检查数据文件\n",
    "        self._check_data_files(source_dir)\n",
    "        \n",
    "        logger.info(\"[INFO] 所有组件初始化完成\")\n",
    "    \n",
    "    def _check_data_files(self, source_dir: str):\n",
    "        \"\"\"检查数据文件\"\"\"\n",
    "        logger.info(\"[INFO] 检查数据文件...\")\n",
    "        \n",
    "        required_files = [\n",
    "            'customers.csv',\n",
    "            'products.csv',\n",
    "            'orders.csv',\n",
    "            'behavior_logs.csv',\n",
    "            'time_dim.csv',\n",
    "            'regions.csv'\n",
    "        ]\n",
    "        \n",
    "        missing_files = []\n",
    "        \n",
    "        for file_name in required_files:\n",
    "            file_path = os.path.join(source_dir, file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "                logger.info(f\"  [OK] {file_name}: {file_size:.2f} MB\")\n",
    "            else:\n",
    "                missing_files.append(file_name)\n",
    "                logger.warning(f\"  [WARNING] {file_name}: 文件不存在\")\n",
    "        \n",
    "        if missing_files:\n",
    "            logger.error(f\"[ERROR] 缺失文件: {', '.join(missing_files)}\")\n",
    "            logger.error(\"[ERROR] 请确保所有CSV文件在当前目录中\")\n",
    "        else:\n",
    "            logger.info(\"[INFO] 所有数据文件检查通过\")\n",
    "    \n",
    "    def run_full_workflow(self):\n",
    "        \"\"\"运行完整的工作流\"\"\"\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\" [WORKFLOW] 开始运行完整ETL工作流\")\n",
    "        logger.info(\"=\"*80)\n",
    "        \n",
    "        workflow_start = datetime.now()\n",
    "        workflow_id = f\"WORKFLOW_{workflow_start.strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        print(f\"\\n [PLAN] 工作流执行计划\")\n",
    "        print(\"-\"*40)\n",
    "        print(\"1.  数据提取\")\n",
    "        print(\"2.  数据清洗和转换\")\n",
    "        print(\"3.  数据加载到数据库\")\n",
    "        print(\"4.  数据质量验证\")\n",
    "        print(\"5.   创建数据仓库视图\")\n",
    "        print(\"6.  数据质量监控\")\n",
    "        print(\"7.  生成报告和仪表板\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        try:\n",
    "            # 步骤1: 运行ETL管道\n",
    "            logger.info(\"\\n[STEP1] 运行ETL管道\")\n",
    "            etl_results = self.etl_pipeline.run_full_pipeline(generate_reports=True)\n",
    "            \n",
    "            if etl_results['status'] != 'success':\n",
    "                logger.error(\"[ERROR] ETL管道运行失败，停止工作流\")\n",
    "                return self._handle_workflow_failure(workflow_id, workflow_start, \"ETL管道失败\")\n",
    "            \n",
    "            # 步骤2: 运行数据质量监控\n",
    "            logger.info(\"\\n[STEP2] 运行数据质量监控\")\n",
    "            monitoring_results = self.quality_monitor.run_comprehensive_monitoring()\n",
    "            \n",
    "            # 步骤3: 计算总体质量评分\n",
    "            logger.info(\"\\n[STEP3] 计算总体质量评分\")\n",
    "            quality_score = self.quality_monitor.get_quality_score()\n",
    "            \n",
    "            # 步骤4: 设置实时监控\n",
    "            logger.info(\"\\n[STEP4] 设置实时监控\")\n",
    "            self.quality_monitor.setup_real_time_monitoring(\n",
    "                interval_minutes=self.schedule_config['monitoring_interval']\n",
    "            )\n",
    "            \n",
    "            # 记录执行历史\n",
    "            workflow_end = datetime.now()\n",
    "            execution_record = {\n",
    "                'workflow_id': workflow_id,\n",
    "                'start_time': workflow_start,\n",
    "                'end_time': workflow_end,\n",
    "                'duration': (workflow_end - workflow_start).total_seconds(),\n",
    "                'etl_status': etl_results['status'],\n",
    "                'quality_score': quality_score,\n",
    "                'monitoring_alerts': monitoring_results['summary']['total_alerts']\n",
    "            }\n",
    "            self.execution_tracker.append(execution_record)\n",
    "            \n",
    "            # 生成工作流报告\n",
    "            self._generate_workflow_report(execution_record, etl_results, monitoring_results)\n",
    "            \n",
    "            # 显示结果\n",
    "            self._display_workflow_summary(execution_record, etl_results, monitoring_results)\n",
    "            \n",
    "            logger.info(f\"[SUCCESS] 完整ETL工作流运行成功!\")\n",
    "            \n",
    "            return {\n",
    "                'workflow_id': workflow_id,\n",
    "                'status': 'success',\n",
    "                'etl_results': etl_results,\n",
    "                'monitoring_results': monitoring_results,\n",
    "                'quality_score': quality_score\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ERROR] 工作流运行失败: {str(e)}\")\n",
    "            return self._handle_workflow_failure(workflow_id, workflow_start, str(e))\n",
    "    \n",
    "    def _handle_workflow_failure(self, workflow_id: str, start_time: datetime, error: str):\n",
    "        \"\"\"处理工作流失败\"\"\"\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        execution_record = {\n",
    "            'workflow_id': workflow_id,\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'duration': (end_time - start_time).total_seconds(),\n",
    "            'status': 'failed',\n",
    "            'error': error\n",
    "        }\n",
    "        self.execution_tracker.append(execution_record)\n",
    "        \n",
    "        # 发送失败通知\n",
    "        if self.schedule_config['notify_on_failure']:\n",
    "            self._send_failure_notification(execution_record)\n",
    "        \n",
    "        logger.error(f\"[ERROR] 工作流执行失败: {error}\")\n",
    "        \n",
    "        return {\n",
    "            'workflow_id': workflow_id,\n",
    "            'status': 'failed',\n",
    "            'error': error\n",
    "        }\n",
    "    \n",
    "    def _generate_workflow_report(self, execution_record: Dict, etl_results: Dict, monitoring_results: Dict):\n",
    "        \"\"\"生成工作流报告\"\"\"\n",
    "        try:\n",
    "            report_data = {\n",
    "                'workflow_summary': execution_record,\n",
    "                'etl_execution': etl_results,\n",
    "                'quality_monitoring': monitoring_results,\n",
    "                'generated_files': self._list_generated_files(),\n",
    "                'recommendations': self._generate_recommendations(monitoring_results),\n",
    "                'generated_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # 保存JSON报告\n",
    "            with open('etl_workflow_report.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "            \n",
    "            # 生成文本报告\n",
    "            self._generate_workflow_text_report(report_data)\n",
    "            \n",
    "            logger.info(f\"[INFO] 工作流报告已生成: etl_workflow_report.json\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ERROR] 生成工作流报告失败: {str(e)}\")\n",
    "    \n",
    "    def _list_generated_files(self) -> List[str]:\n",
    "        \"\"\"列出生成的文件\"\"\"\n",
    "        generated_files = []\n",
    "        \n",
    "        # 检查ETL生成的文件\n",
    "        etl_files = ['etl_execution_report.json', 'etl_report.txt', 'etl_dashboard.html']\n",
    "        for file in etl_files:\n",
    "            if os.path.exists(file):\n",
    "                generated_files.append(file)\n",
    "        \n",
    "        # 检查监控生成的文件\n",
    "        monitor_files = ['data_quality_monitoring_report.json', 'data_quality_report.txt', \n",
    "                        'data_quality_dashboard.html']\n",
    "        for file in monitor_files:\n",
    "            if os.path.exists(file):\n",
    "                generated_files.append(file)\n",
    "        \n",
    "        # 检查清洗后的数据文件\n",
    "        if os.path.exists('cleaned_data'):\n",
    "            for file in os.listdir('cleaned_data'):\n",
    "                if file.endswith('.csv'):\n",
    "                    generated_files.append(f'cleaned_data/{file}')\n",
    "        \n",
    "        return generated_files\n",
    "    \n",
    "    def _generate_recommendations(self, monitoring_results: Dict) -> List[str]:\n",
    "        \"\"\"生成改进建议\"\"\"\n",
    "        recommendations = []\n",
    "        alerts = monitoring_results.get('alerts', [])\n",
    "        \n",
    "        # 分析告警生成建议\n",
    "        error_alerts = [a for a in alerts if a['severity'] == 'error']\n",
    "        warning_alerts = [a for a in alerts if a['severity'] == 'warning']\n",
    "        \n",
    "        if error_alerts:\n",
    "            recommendations.append(\"立即处理严重告警，确保数据准确性\")\n",
    "        \n",
    "        if warning_alerts:\n",
    "            recommendations.append(\"定期检查警告告警，预防数据质量问题\")\n",
    "        \n",
    "        # 检查缺失值\n",
    "        missing_alerts = [a for a in alerts if a['type'] == 'missing_values']\n",
    "        if missing_alerts:\n",
    "            recommendations.append(\"考虑实施缺失值填充策略\")\n",
    "        \n",
    "        # 检查重复数据\n",
    "        duplicate_alerts = [a for a in alerts if a['type'] == 'duplicate_rows']\n",
    "        if duplicate_alerts:\n",
    "            recommendations.append(\"实施数据去重流程，定期清理重复数据\")\n",
    "        \n",
    "        # 检查引用完整性\n",
    "        integrity_alerts = [a for a in alerts if a['type'] == 'referential_integrity']\n",
    "        if integrity_alerts:\n",
    "            recommendations.append(\"修复引用完整性错误，确保数据一致性\")\n",
    "        \n",
    "        # 通用建议\n",
    "        recommendations.append(\"定期备份清洗后的数据\")\n",
    "        recommendations.append(\"建立数据质量监控计划\")\n",
    "        recommendations.append(\"培训团队数据质量标准和最佳实践\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _generate_workflow_text_report(self, report_data: Dict):\n",
    "        \"\"\"生成工作流文本报告\"\"\"\n",
    "        report_lines = []\n",
    "        report_lines.append(\"=\"*80)\n",
    "        report_lines.append(\" [REPORT] ETL工作流执行报告\")\n",
    "        report_lines.append(f\"生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report_lines.append(\"=\"*80)\n",
    "        \n",
    "        # 工作流摘要\n",
    "        summary = report_data['workflow_summary']\n",
    "        report_lines.append(f\"\\n [SUMMARY] 工作流摘要\")\n",
    "        report_lines.append(f\"-\"*40)\n",
    "        report_lines.append(f\"工作流ID: {summary['workflow_id']}\")\n",
    "        report_lines.append(f\"状态: {summary.get('status', 'completed')}\")\n",
    "        report_lines.append(f\"开始时间: {summary['start_time']}\")\n",
    "        report_lines.append(f\"结束时间: {summary['end_time']}\")\n",
    "        report_lines.append(f\"总耗时: {summary['duration']:.2f} 秒\")\n",
    "        \n",
    "        if 'quality_score' in summary:\n",
    "            report_lines.append(f\"数据质量评分: {summary['quality_score']:.1f}/100\")\n",
    "        \n",
    "        if 'monitoring_alerts' in summary:\n",
    "            report_lines.append(f\"监控告警数: {summary['monitoring_alerts']}\")\n",
    "        \n",
    "        # 生成的文件\n",
    "        generated_files = report_data.get('generated_files', [])\n",
    "        report_lines.append(f\"\\n [FILES] 生成的文件\")\n",
    "        report_lines.append(f\"-\"*40)\n",
    "        \n",
    "        if generated_files:\n",
    "            for file in generated_files:\n",
    "                report_lines.append(f\"  [OK] {file}\")\n",
    "        else:\n",
    "            report_lines.append(\"  未生成文件\")\n",
    "        \n",
    "        # 改进建议\n",
    "        recommendations = report_data.get('recommendations', [])\n",
    "        if recommendations:\n",
    "            report_lines.append(f\"\\n [RECOMMEND] 改进建议\")\n",
    "            report_lines.append(f\"-\"*40)\n",
    "            for i, rec in enumerate(recommendations, 1):\n",
    "                report_lines.append(f\"{i}. {rec}\")\n",
    "        \n",
    "        # 保存文本报告\n",
    "        with open('etl_workflow_report.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(report_lines))\n",
    "    \n",
    "    def _display_workflow_summary(self, execution_record: Dict, etl_results: Dict, monitoring_results: Dict):\n",
    "        \"\"\"显示工作流摘要\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" [COMPLETED] ETL工作流执行完成!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\n [SUMMARY] 执行摘要\")\n",
    "        print(\"-\"*40)\n",
    "        print(f\"工作流ID: {execution_record['workflow_id']}\")\n",
    "        print(f\"执行状态: 成功\")\n",
    "        print(f\"总耗时: {execution_record['duration']:.2f} 秒\")\n",
    "        \n",
    "        if 'quality_score' in execution_record:\n",
    "            score = execution_record['quality_score']\n",
    "            if score >= 80:\n",
    "                rating = \"[PASS]\"\n",
    "            elif score >= 60:\n",
    "                rating = \"[WARN]\"\n",
    "            else:\n",
    "                rating = \"[FAIL]\"\n",
    "            print(f\"数据质量评分: {rating} {score:.1f}/100\")\n",
    "        \n",
    "        # ETL结果\n",
    "        etl_stages = etl_results.get('stages', {})\n",
    "        print(f\"\\n [ETL] ETL管道结果\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        for stage_name, stage_info in etl_stages.items():\n",
    "            duration = stage_info.get('duration', 0)\n",
    "            status = stage_info.get('status', 'unknown')\n",
    "            if status == 'completed':\n",
    "                status_icon = '[OK]'\n",
    "            elif status == 'partial':\n",
    "                status_icon = '[PARTIAL]'\n",
    "            else:\n",
    "                status_icon = '[FAIL]'\n",
    "            print(f\"{status_icon} {stage_name:15s}: {duration:6.2f} 秒\")\n",
    "        \n",
    "        # 监控结果\n",
    "        monitoring_summary = monitoring_results.get('summary', {})\n",
    "        print(f\"\\n [MONITOR] 数据质量监控结果\")\n",
    "        print(\"-\"*40)\n",
    "        print(f\"总告警数: {monitoring_summary.get('total_alerts', 0)}\")\n",
    "        print(f\"严重告警:  {monitoring_summary.get('critical_alerts', 0)}\")\n",
    "        print(f\"警告告警:  {monitoring_summary.get('warning_alerts', 0)}\")\n",
    "        print(f\"信息告警:  {monitoring_summary.get('info_alerts', 0)}\")\n",
    "        \n",
    "        # 生成的文件\n",
    "        print(f\"\\n [FILES] 生成的文件和报告\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        generated_files = self._list_generated_files()\n",
    "        if generated_files:\n",
    "            categories = {}\n",
    "            for file in generated_files:\n",
    "                if 'etl' in file:\n",
    "                    categories.setdefault('ETL报告', []).append(file)\n",
    "                elif 'quality' in file:\n",
    "                    categories.setdefault('质量报告', []).append(file)\n",
    "                elif 'dashboard' in file:\n",
    "                    categories.setdefault('仪表板', []).append(file)\n",
    "                elif 'cleaned_data' in file:\n",
    "                    categories.setdefault('清洗数据', []).append(file)\n",
    "                else:\n",
    "                    categories.setdefault('其他', []).append(file)\n",
    "            \n",
    "            for category, files in categories.items():\n",
    "                print(f\"\\n{category}:\")\n",
    "                for file in files:\n",
    "                    print(f\"  [OK] {file}\")\n",
    "        else:\n",
    "            print(\"未生成文件\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" [SUCCESS] 所有任务完成! 建议查看生成的报告和仪表板\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def _send_failure_notification(self, execution_record: Dict):\n",
    "        \"\"\"发送失败通知（模拟）\"\"\"\n",
    "        notification = f\"\"\"\n",
    "        [ALERT] ETL工作流失败通知\n",
    "        ====================\n",
    "        工作流ID: {execution_record['workflow_id']}\n",
    "        失败时间: {execution_record['end_time']}\n",
    "        错误信息: {execution_record['error']}\n",
    "        持续时间: {execution_record['duration']:.2f} 秒\n",
    "        \n",
    "        请立即检查:\n",
    "        1. 数据源文件是否存在\n",
    "        2. 数据库连接是否正常\n",
    "        3. 查看 etl_pipeline.log 获取详细错误信息\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.error(\"[ERROR] 发送失败通知（模拟）:\")\n",
    "        print(notification)\n",
    "    \n",
    "    def schedule_daily_etl(self, hour: int = 2, minute: int = 0):\n",
    "        \"\"\"安排每日ETL任务（模拟）\"\"\"\n",
    "        logger.info(f\"[INFO] 安排每日ETL任务，时间: {hour:02d}:{minute:02d}\")\n",
    "        \n",
    "        # 在实际应用中，这里会使用调度器如APScheduler或cron\n",
    "        # 这里只是模拟\n",
    "        logger.info(\"[INFO] 每日调度已设置（模拟模式）\")\n",
    "        logger.info(f\"[INFO] ETL任务将每天 {hour:02d}:{minute:02d} 自动运行\")\n",
    "    \n",
    "    def get_execution_history(self) -> pd.DataFrame:\n",
    "        \"\"\"获取执行历史\"\"\"\n",
    "        if self.execution_tracker:\n",
    "            return pd.DataFrame(self.execution_tracker)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "logger.info(\"[OK] ETL编排器类定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458eefe6-c806-4eb5-aa66-67c58f2ca7b3",
   "metadata": {},
   "source": [
    "# 5.运行ETL工作流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8db97fba-fc47-4bce-92b2-b63cfea10714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:41:26,974 - ETL_Pipeline - INFO - [INFO] ETL编排器初始化完成\n",
      "2025-12-02 17:41:26,976 - ETL_Pipeline - INFO - [INFO] 初始化ETL组件...\n",
      "2025-12-02 17:41:26,977 - ETL_Pipeline - INFO - ETL管道初始化 - 源目录: ., 数据库: ecommerce_clean.db\n",
      "2025-12-02 17:41:26,977 - ETL_Pipeline - INFO - [OK] ETL管道初始化完成\n",
      "2025-12-02 17:41:26,978 - ETL_Pipeline - INFO - [INFO] 数据质量监控器初始化 - 数据库: ecommerce_clean.db\n",
      "2025-12-02 17:41:26,979 - ETL_Pipeline - INFO - [OK] 数据质量监控器初始化完成\n",
      "2025-12-02 17:41:26,980 - ETL_Pipeline - INFO - [INFO] 检查数据文件...\n",
      "2025-12-02 17:41:26,981 - ETL_Pipeline - INFO -   [OK] customers.csv: 0.69 MB\n",
      "2025-12-02 17:41:26,982 - ETL_Pipeline - INFO -   [OK] products.csv: 0.02 MB\n",
      "2025-12-02 17:41:26,984 - ETL_Pipeline - INFO -   [OK] orders.csv: 4.30 MB\n",
      "2025-12-02 17:41:26,985 - ETL_Pipeline - INFO -   [OK] behavior_logs.csv: 6.71 MB\n",
      "2025-12-02 17:41:26,986 - ETL_Pipeline - INFO -   [OK] time_dim.csv: 0.05 MB\n",
      "2025-12-02 17:41:26,987 - ETL_Pipeline - INFO -   [OK] regions.csv: 0.00 MB\n",
      "2025-12-02 17:41:26,988 - ETL_Pipeline - INFO - [INFO] 所有数据文件检查通过\n",
      "2025-12-02 17:41:26,989 - ETL_Pipeline - INFO - [INFO] 所有组件初始化完成\n",
      "2025-12-02 17:41:26,990 - ETL_Pipeline - INFO - ================================================================================\n",
      "2025-12-02 17:41:26,991 - ETL_Pipeline - INFO -  [WORKFLOW] 开始运行完整ETL工作流\n",
      "2025-12-02 17:41:26,992 - ETL_Pipeline - INFO - ================================================================================\n",
      "2025-12-02 17:41:26,993 - ETL_Pipeline - INFO - \n",
      "[STEP1] 运行ETL管道\n",
      "2025-12-02 17:41:26,994 - ETL_Pipeline - INFO - ================================================================================\n",
      "2025-12-02 17:41:26,994 - ETL_Pipeline - INFO -  [START] 开始运行完整ETL管道\n",
      "2025-12-02 17:41:26,995 - ETL_Pipeline - INFO - ================================================================================\n",
      "2025-12-02 17:41:26,996 - ETL_Pipeline - INFO - [阶段1] 数据提取\n",
      "2025-12-02 17:41:26,997 - ETL_Pipeline - INFO - [INFO] 开始数据提取...\n",
      "2025-12-02 17:41:27,049 - ETL_Pipeline - INFO -    [OK] 已提取: customers.csv (5,000 行)\n",
      "2025-12-02 17:41:27,055 - ETL_Pipeline - INFO -    [OK] 已提取: products.csv (200 行)\n",
      "2025-12-02 17:41:27,064 - ETL_Pipeline - INFO -    [OK] 已提取: time_dim.csv (1,096 行)\n",
      "2025-12-02 17:41:27,068 - ETL_Pipeline - INFO -    [OK] 已提取: regions.csv (6 行)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] 开始运行完整的ETL工作流\n",
      "================================================================================\n",
      "1. [STEP1] 初始化ETL编排器...\n",
      "2. [STEP2] 初始化ETL组件...\n",
      "3. [STEP3] 运行完整ETL工作流...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " [PLAN] 工作流执行计划\n",
      "----------------------------------------\n",
      "1.  数据提取\n",
      "2.  数据清洗和转换\n",
      "3.  数据加载到数据库\n",
      "4.  数据质量验证\n",
      "5.   创建数据仓库视图\n",
      "6.  数据质量监控\n",
      "7.  生成报告和仪表板\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:41:27,231 - ETL_Pipeline - INFO -    [OK] 已提取: orders.csv (50,150 行)\n",
      "2025-12-02 17:41:27,492 - ETL_Pipeline - INFO -    [OK] 已提取: behavior_logs.csv (100,500 行)\n",
      "2025-12-02 17:41:27,494 - ETL_Pipeline - INFO - [INFO] 数据提取完成，共加载 6 个表\n",
      "2025-12-02 17:41:27,495 - ETL_Pipeline - INFO - [阶段2] 数据清洗和转换\n",
      "2025-12-02 17:41:27,496 - ETL_Pipeline - INFO - [INFO] 开始数据清洗和转换...\n",
      "2025-12-02 17:41:27,497 - ETL_Pipeline - INFO - [INFO] 清洗表: customers\n",
      "2025-12-02 17:41:27,591 - ETL_Pipeline - INFO -    [OK] 清洗完成: 5,000 → 4,911 行 (移除: 1.8%)\n",
      "2025-12-02 17:41:27,593 - ETL_Pipeline - INFO - [INFO] 清洗表: products\n",
      "2025-12-02 17:41:27,607 - ETL_Pipeline - INFO -    [OK] 清洗完成: 200 → 200 行 (移除: 0.0%)\n",
      "2025-12-02 17:41:27,608 - ETL_Pipeline - INFO - [INFO] 清洗表: time_dim\n",
      "2025-12-02 17:41:27,619 - ETL_Pipeline - INFO -    [OK] 清洗完成: 1,096 → 1,096 行 (移除: 0.0%)\n",
      "2025-12-02 17:41:27,620 - ETL_Pipeline - INFO - [INFO] 清洗表: regions\n",
      "2025-12-02 17:41:27,624 - ETL_Pipeline - INFO -    [OK] 清洗完成: 6 → 6 行 (移除: 0.0%)\n",
      "2025-12-02 17:41:27,625 - ETL_Pipeline - INFO - [INFO] 清洗表: orders\n",
      "2025-12-02 17:41:28,045 - ETL_Pipeline - INFO -    [OK] 清洗完成: 50,150 → 49,505 行 (移除: 1.3%)\n",
      "2025-12-02 17:41:28,046 - ETL_Pipeline - INFO - [INFO] 清洗表: behavior_logs\n",
      "2025-12-02 17:41:28,536 - ETL_Pipeline - INFO -    [OK] 清洗完成: 100,500 → 100,000 行 (移除: 0.5%)\n",
      "2025-12-02 17:41:28,537 - ETL_Pipeline - INFO - [INFO] 数据清洗完成，共处理 6 个表\n",
      "2025-12-02 17:41:28,537 - ETL_Pipeline - INFO - [阶段3] 数据加载\n",
      "2025-12-02 17:41:28,538 - ETL_Pipeline - INFO - [INFO] 开始加载数据到数据库...\n",
      "2025-12-02 17:41:28,540 - ETL_Pipeline - INFO - [INFO] 成功连接到数据库: ecommerce_clean.db\n",
      "2025-12-02 17:41:28,542 - ETL_Pipeline - INFO - [INFO] 数据库表创建完成\n",
      "2025-12-02 17:41:28,659 - ETL_Pipeline - INFO -    [OK] 已加载: customers (4,911 行)\n",
      "2025-12-02 17:41:28,689 - ETL_Pipeline - INFO -    [OK] 已加载: products (200 行)\n",
      "2025-12-02 17:41:28,725 - ETL_Pipeline - INFO -    [OK] 已加载: time_dim (1,096 行)\n",
      "2025-12-02 17:41:28,748 - ETL_Pipeline - INFO -    [OK] 已加载: regions (6 行)\n",
      "2025-12-02 17:41:29,570 - ETL_Pipeline - INFO -    [OK] 已加载: orders (49,505 行)\n",
      "2025-12-02 17:41:30,304 - ETL_Pipeline - INFO -    [OK] 已加载: behavior_logs (100,000 行)\n",
      "2025-12-02 17:41:30,305 - ETL_Pipeline - INFO - [INFO] 数据加载完成\n",
      "2025-12-02 17:41:30,306 - ETL_Pipeline - INFO - [阶段4] 数据质量验证\n",
      "2025-12-02 17:41:30,306 - ETL_Pipeline - INFO - [INFO] 开始数据质量验证...\n",
      "2025-12-02 17:41:31,330 - ETL_Pipeline - INFO - [INFO] 质量验证完成 - 总体评分: 91.7/100\n",
      "2025-12-02 17:41:31,331 - ETL_Pipeline - INFO - [阶段5] 数据仓库视图创建\n",
      "2025-12-02 17:41:31,332 - ETL_Pipeline - INFO - [INFO] 数据仓库视图创建完成\n",
      "2025-12-02 17:41:31,333 - ETL_Pipeline - INFO - [SUCCESS] ETL管道运行成功! 总耗时: 4.34秒\n",
      "2025-12-02 17:41:31,337 - ETL_Pipeline - INFO - [INFO] 执行报告已生成: etl_execution_report.json\n",
      "2025-12-02 17:41:31,342 - ETL_Pipeline - ERROR - [ERROR] 生成仪表板失败: Unsupported subplot type: 'gauge'\n",
      "2025-12-02 17:41:31,360 - ETL_Pipeline - INFO - \n",
      "[STEP2] 运行数据质量监控\n",
      "2025-12-02 17:41:31,361 - ETL_Pipeline - INFO - ================================================================================\n",
      "2025-12-02 17:41:31,362 - ETL_Pipeline - INFO -  [MONITOR] 开始全面数据质量监控\n",
      "2025-12-02 17:41:31,363 - ETL_Pipeline - INFO - ================================================================================\n",
      "2025-12-02 17:41:31,366 - ETL_Pipeline - INFO - [INFO] 监控器成功连接到数据库: ecommerce_clean.db\n",
      "2025-12-02 17:41:31,368 - ETL_Pipeline - INFO - [INFO] 监控表: customers\n",
      "2025-12-02 17:41:31,447 - ETL_Pipeline - WARNING - [WARNING] customers: 缺失值比例 6.02% 超过阈值 5.0%\n",
      "2025-12-02 17:41:31,453 - ETL_Pipeline - INFO - [INFO] customers.total_spent: 发现 134 个异常值 (2.73%)\n",
      "2025-12-02 17:41:31,457 - ETL_Pipeline - INFO - [INFO] 监控表: products\n",
      "2025-12-02 17:41:31,469 - ETL_Pipeline - INFO - [INFO] 监控表: time_dim\n",
      "2025-12-02 17:41:31,483 - ETL_Pipeline - INFO - [INFO] 监控表: regions\n",
      "2025-12-02 17:41:31,487 - ETL_Pipeline - INFO - [INFO] 监控表: orders\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " ETL管道执行报告\n",
      "生成时间: 2025-12-02 17:41:31\n",
      "================================================================================\n",
      "\n",
      " [SUMMARY] 执行摘要\n",
      "----------------------------------------\n",
      "执行ID: ETL_20251202_174126\n",
      "状态: success\n",
      "开始时间: 2025-12-02T17:41:26.996870\n",
      "结束时间: 2025-12-02T17:41:31.333903\n",
      "总耗时: 4.34 秒\n",
      "处理表数: 6\n",
      "\n",
      " [STAGES] 阶段详情\n",
      "----------------------------------------\n",
      "extraction     : completed  | 耗时: 0.50秒\n",
      "transformation : completed  | 耗时: 1.04秒\n",
      "loading        : completed  | 耗时: 1.77秒\n",
      "validation     : completed  | 耗时: 1.02秒\n",
      "warehouse      : completed  | 耗时: 0.00秒\n",
      "\n",
      " [QUALITY] 数据质量报告\n",
      "----------------------------------------\n",
      "总体质量评分: 91.7/100\n",
      "发现问题总数: 5\n",
      "\n",
      " 各表质量:\n",
      "  customers      :  90.0/100 | 问题数: 1\n",
      "  products       :  90.0/100 | 问题数: 1\n",
      "  orders         :  90.0/100 | 问题数: 1\n",
      "  behavior_logs  :  90.0/100 | 问题数: 1\n",
      "  time_dim       :  90.0/100 | 问题数: 1\n",
      "  regions        : 100.0/100 | 问题数: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:41:32,037 - ETL_Pipeline - ERROR - [ERROR] orders.browsing_duration_seconds: 发现 245 个负值\n",
      "2025-12-02 17:41:32,039 - ETL_Pipeline - ERROR - [ERROR] orders.click_count: 发现 198 个负值\n",
      "2025-12-02 17:41:32,063 - ETL_Pipeline - INFO - [INFO] 监控表: behavior_logs\n",
      "2025-12-02 17:41:32,690 - ETL_Pipeline - WARNING - [WARNING] behavior_logs: 缺失值比例 6.99% 超过阈值 5.0%\n",
      "2025-12-02 17:41:32,752 - ETL_Pipeline - ERROR - [ERROR] 订单表引用了 1457 个不存在的客户ID\n",
      "2025-12-02 17:41:32,782 - ETL_Pipeline - ERROR - [ERROR] 订单表引用了 141 个不存在的产品ID\n",
      "2025-12-02 17:41:32,856 - ETL_Pipeline - ERROR - [ERROR] 行为日志表引用了 3490 个不存在的客户ID\n",
      "2025-12-02 17:41:32,899 - ETL_Pipeline - ERROR - [ERROR] 生成监控可视化失败: Unsupported subplot type: 'line'\n",
      "2025-12-02 17:41:32,900 - ETL_Pipeline - INFO - [INFO] 监控报告已生成: data_quality_monitoring_report.json\n",
      "2025-12-02 17:41:32,901 - ETL_Pipeline - INFO - [INFO] 监控完成 - 发现告警: 8 个\n",
      "2025-12-02 17:41:32,903 - ETL_Pipeline - INFO - \n",
      "[STEP3] 计算总体质量评分\n",
      "2025-12-02 17:41:32,905 - ETL_Pipeline - INFO - ================================================================================\n",
      "2025-12-02 17:41:32,906 - ETL_Pipeline - INFO -  [MONITOR] 开始全面数据质量监控\n",
      "2025-12-02 17:41:32,907 - ETL_Pipeline - INFO - ================================================================================\n",
      "2025-12-02 17:41:32,908 - ETL_Pipeline - INFO - [INFO] 监控表: customers\n",
      "2025-12-02 17:41:33,001 - ETL_Pipeline - WARNING - [WARNING] customers: 缺失值比例 6.02% 超过阈值 5.0%\n",
      "2025-12-02 17:41:33,013 - ETL_Pipeline - INFO - [INFO] customers.total_spent: 发现 134 个异常值 (2.73%)\n",
      "2025-12-02 17:41:33,019 - ETL_Pipeline - INFO - [INFO] 监控表: products\n",
      "2025-12-02 17:41:33,035 - ETL_Pipeline - INFO - [INFO] 监控表: time_dim\n",
      "2025-12-02 17:41:33,055 - ETL_Pipeline - INFO - [INFO] 监控表: regions\n",
      "2025-12-02 17:41:33,060 - ETL_Pipeline - INFO - [INFO] 监控表: orders\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " [REPORT] 数据质量监控报告\n",
      "生成时间: 2025-12-02 17:41:32\n",
      "================================================================================\n",
      "\n",
      " [SUMMARY] 监控摘要\n",
      "----------------------------------------\n",
      "监控表数: 6\n",
      "总告警数: 8\n",
      "严重告警: 5\n",
      "警告告警: 2\n",
      "信息告警: 1\n",
      "\n",
      " [ALERTS] 告警详情\n",
      "----------------------------------------\n",
      "\n",
      "[严重告警] (5 个):\n",
      "  • orders.browsing_duration_seconds: 发现 245 个负值\n",
      "  • orders.click_count: 发现 198 个负值\n",
      "  • 订单表引用了 1457 个不存在的客户ID\n",
      "  • 订单表引用了 141 个不存在的产品ID\n",
      "  • 行为日志表引用了 3490 个不存在的客户ID\n",
      "\n",
      "[警告告警] (2 个):\n",
      "  • customers: 缺失值比例 6.02% 超过阈值 5.0%\n",
      "  • behavior_logs: 缺失值比例 6.99% 超过阈值 5.0%\n",
      "\n",
      "[信息告警] (1 个):\n",
      "  • customers.total_spent: 发现 134 个异常值 (2.73%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:41:33,707 - ETL_Pipeline - ERROR - [ERROR] orders.browsing_duration_seconds: 发现 245 个负值\n",
      "2025-12-02 17:41:33,709 - ETL_Pipeline - ERROR - [ERROR] orders.click_count: 发现 198 个负值\n",
      "2025-12-02 17:41:33,731 - ETL_Pipeline - INFO - [INFO] 监控表: behavior_logs\n",
      "2025-12-02 17:41:34,418 - ETL_Pipeline - WARNING - [WARNING] behavior_logs: 缺失值比例 6.99% 超过阈值 5.0%\n",
      "2025-12-02 17:41:34,490 - ETL_Pipeline - ERROR - [ERROR] 订单表引用了 1457 个不存在的客户ID\n",
      "2025-12-02 17:41:34,526 - ETL_Pipeline - ERROR - [ERROR] 订单表引用了 141 个不存在的产品ID\n",
      "2025-12-02 17:41:34,606 - ETL_Pipeline - ERROR - [ERROR] 行为日志表引用了 3490 个不存在的客户ID\n",
      "2025-12-02 17:41:34,647 - ETL_Pipeline - ERROR - [ERROR] 生成监控可视化失败: Unsupported subplot type: 'line'\n",
      "2025-12-02 17:41:34,648 - ETL_Pipeline - INFO - [INFO] 监控报告已生成: data_quality_monitoring_report.json\n",
      "2025-12-02 17:41:34,649 - ETL_Pipeline - INFO - [INFO] 监控完成 - 发现告警: 8 个\n",
      "2025-12-02 17:41:34,650 - ETL_Pipeline - INFO - \n",
      "[STEP4] 设置实时监控\n",
      "2025-12-02 17:41:34,650 - ETL_Pipeline - INFO - [INFO] 设置实时监控，每 60 分钟运行一次\n",
      "2025-12-02 17:41:34,651 - ETL_Pipeline - INFO - [INFO] 实时监控已设置（模拟模式）\n",
      "2025-12-02 17:41:34,652 - ETL_Pipeline - INFO - [INFO] 下次监控将在 60 分钟后运行\n",
      "2025-12-02 17:41:34,655 - ETL_Pipeline - INFO - [INFO] 工作流报告已生成: etl_workflow_report.json\n",
      "2025-12-02 17:41:34,657 - ETL_Pipeline - INFO - [SUCCESS] 完整ETL工作流运行成功!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " [REPORT] 数据质量监控报告\n",
      "生成时间: 2025-12-02 17:41:34\n",
      "================================================================================\n",
      "\n",
      " [SUMMARY] 监控摘要\n",
      "----------------------------------------\n",
      "监控表数: 6\n",
      "总告警数: 8\n",
      "严重告警: 5\n",
      "警告告警: 2\n",
      "信息告警: 1\n",
      "\n",
      " [ALERTS] 告警详情\n",
      "----------------------------------------\n",
      "\n",
      "[严重告警] (5 个):\n",
      "  • orders.browsing_duration_seconds: 发现 245 个负值\n",
      "  • orders.click_count: 发现 198 个负值\n",
      "  • 订单表引用了 1457 个不存在的客户ID\n",
      "  • 订单表引用了 141 个不存在的产品ID\n",
      "  • 行为日志表引用了 3490 个不存在的客户ID\n",
      "\n",
      "[警告告警] (2 个):\n",
      "  • customers: 缺失值比例 6.02% 超过阈值 5.0%\n",
      "  • behavior_logs: 缺失值比例 6.99% 超过阈值 5.0%\n",
      "\n",
      "[信息告警] (1 个):\n",
      "  • customers.total_spent: 发现 134 个异常值 (2.73%)\n",
      "\n",
      "================================================================================\n",
      " [COMPLETED] ETL工作流执行完成!\n",
      "================================================================================\n",
      "\n",
      " [SUMMARY] 执行摘要\n",
      "----------------------------------------\n",
      "工作流ID: WORKFLOW_20251202_174126\n",
      "执行状态: 成功\n",
      "总耗时: 7.66 秒\n",
      "数据质量评分: [FAIL] 44.0/100\n",
      "\n",
      " [ETL] ETL管道结果\n",
      "----------------------------------------\n",
      "[OK] extraction     :   0.50 秒\n",
      "[OK] transformation :   1.04 秒\n",
      "[OK] loading        :   1.77 秒\n",
      "[OK] validation     :   1.02 秒\n",
      "[OK] warehouse      :   0.00 秒\n",
      "\n",
      " [MONITOR] 数据质量监控结果\n",
      "----------------------------------------\n",
      "总告警数: 8\n",
      "严重告警:  5\n",
      "警告告警:  2\n",
      "信息告警:  1\n",
      "\n",
      " [FILES] 生成的文件和报告\n",
      "----------------------------------------\n",
      "\n",
      "ETL报告:\n",
      "  [OK] etl_execution_report.json\n",
      "  [OK] etl_report.txt\n",
      "\n",
      "质量报告:\n",
      "  [OK] data_quality_monitoring_report.json\n",
      "  [OK] data_quality_report.txt\n",
      "\n",
      "清洗数据:\n",
      "  [OK] cleaned_data/cleaned_behavior_logs.csv\n",
      "  [OK] cleaned_data/cleaned_customers.csv\n",
      "  [OK] cleaned_data/cleaned_orders.csv\n",
      "  [OK] cleaned_data/cleaned_products.csv\n",
      "  [OK] cleaned_data/cleaned_regions.csv\n",
      "  [OK] cleaned_data/cleaned_time_dim.csv\n",
      "\n",
      "================================================================================\n",
      " [SUCCESS] 所有任务完成! 建议查看生成的报告和仪表板\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "[COMPLETED] ETL工作流执行完成\n",
      "================================================================================\n",
      "\n",
      "[SUCCESS] 工作流执行成功!\n",
      "\n",
      "[METRICS] 关键指标:\n",
      "  数据质量评分: 44.0/100 (需要改进 [FIX])\n",
      "\n",
      "[FILES] 生成的文件:\n",
      "  [OK] cleaned_behavior_logs.csv: 6.70 MB\n",
      "  [OK] cleaned_customers.csv: 0.72 MB\n",
      "  [OK] cleaned_orders.csv: 4.32 MB\n",
      "  [OK] cleaned_products.csv: 0.02 MB\n",
      "  [OK] cleaned_regions.csv: 0.00 MB\n",
      "  [OK] cleaned_time_dim.csv: 0.05 MB\n",
      "\n",
      "[REPORTS] 生成的报告:\n",
      "  [OK] etl_report.txt\n",
      "  [OK] data_quality_report.txt\n",
      "  [OK] etl_workflow_report.txt\n",
      "\n",
      "[DASHBOARDS] 生成的仪表板:\n",
      "\n",
      "[DATABASE] 生成的数据库: ecommerce_clean.db\n",
      "\n",
      "[TIPS] 下一步建议:\n",
      "  1. 查看 etl_dashboard.html 了解ETL执行详情\n",
      "  2. 查看 data_quality_dashboard.html 监控数据质量\n",
      "  3. 使用清洗后的数据进行业务分析\n",
      "  4. 定期运行数据质量监控\n",
      "\n",
      "================================================================================\n",
      "[COMPLETED] ETL管道架构和数据质量监控系统已部署完成!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 代码块5：运行完整的ETL工作流\n",
    "print(\"[START] 开始运行完整的ETL工作流\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. 初始化编排器\n",
    "print(\"1. [STEP1] 初始化ETL编排器...\")\n",
    "orchestrator = ETLOrchestrator()\n",
    "\n",
    "# 2. 初始化组件\n",
    "print(\"2. [STEP2] 初始化ETL组件...\")\n",
    "orchestrator.initialize_components(source_dir='.', db_path='ecommerce_clean.db')\n",
    "\n",
    "# 3. 运行完整工作流\n",
    "print(\"3. [STEP3] 运行完整ETL工作流...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "workflow_results = orchestrator.run_full_workflow()\n",
    "\n",
    "# 4. 显示最终结果\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[COMPLETED] ETL工作流执行完成\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if workflow_results.get('status') == 'success':\n",
    "    print(\"\\n[SUCCESS] 工作流执行成功!\")\n",
    "    \n",
    "    # 显示关键指标\n",
    "    quality_score = workflow_results.get('quality_score', 0)\n",
    "    if quality_score >= 90:\n",
    "        rating = \"优秀 [TOP]\"\n",
    "    elif quality_score >= 75:\n",
    "        rating = \"良好 [GOOD]\"\n",
    "    elif quality_score >= 60:\n",
    "        rating = \"一般 [WARN]\"\n",
    "    else:\n",
    "        rating = \"需要改进 [FIX]\"\n",
    "    \n",
    "    print(f\"\\n[METRICS] 关键指标:\")\n",
    "    print(f\"  数据质量评分: {quality_score:.1f}/100 ({rating})\")\n",
    "    \n",
    "    # 检查生成的文件\n",
    "    print(f\"\\n[FILES] 生成的文件:\")\n",
    "    if os.path.exists('cleaned_data'):\n",
    "        csv_files = [f for f in os.listdir('cleaned_data') if f.endswith('.csv')]\n",
    "        for file in csv_files:\n",
    "            file_path = os.path.join('cleaned_data', file)\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"  [OK] {file}: {size_mb:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\n[REPORTS] 生成的报告:\")\n",
    "    report_files = ['etl_report.txt', 'data_quality_report.txt', \n",
    "                   'etl_workflow_report.txt']\n",
    "    for file in report_files:\n",
    "        if os.path.exists(file):\n",
    "            print(f\"  [OK] {file}\")\n",
    "    \n",
    "    print(f\"\\n[DASHBOARDS] 生成的仪表板:\")\n",
    "    dashboard_files = ['etl_dashboard.html', 'data_quality_dashboard.html']\n",
    "    for file in dashboard_files:\n",
    "        if os.path.exists(file):\n",
    "            print(f\"  [OK] {file}\")\n",
    "    \n",
    "    print(f\"\\n[DATABASE] 生成的数据库: ecommerce_clean.db\")\n",
    "    \n",
    "    # 下一步建议\n",
    "    print(f\"\\n[TIPS] 下一步建议:\")\n",
    "    print(\"  1. 查看 etl_dashboard.html 了解ETL执行详情\")\n",
    "    print(\"  2. 查看 data_quality_dashboard.html 监控数据质量\")\n",
    "    print(\"  3. 使用清洗后的数据进行业务分析\")\n",
    "    print(\"  4. 定期运行数据质量监控\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n[FAILURE] 工作流执行失败!\")\n",
    "    print(f\"   错误: {workflow_results.get('error', '未知错误')}\")\n",
    "    print(f\"\\n[TROUBLESHOOTING] 故障排除:\")\n",
    "    print(\"  1. 检查所有CSV文件是否在当前目录\")\n",
    "    print(\"  2. 查看 etl_pipeline.log 获取详细错误信息\")\n",
    "    print(\"  3. 确保有足够的磁盘空间\")\n",
    "    print(\"  4. 检查数据库文件权限\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[COMPLETED] ETL管道架构和数据质量监控系统已部署完成!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a906fe55-cc11-4e9e-9bda-c5a79ed2c10f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
